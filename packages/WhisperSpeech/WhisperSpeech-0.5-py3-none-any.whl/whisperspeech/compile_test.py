# AUTOGENERATED! DO NOT EDIT! File to edit: ../nbs/Model inference tests-Copy1.ipynb.

# %% auto 0
__all__ = []

# %% ../nbs/Model inference tests-Copy1.ipynb 1
import os
# os.environ['TORCH_LOGS'] = ""
# os.environ['TORCH_LOGS'] = "+guards,+perf_hints,+dynamo"
# os.environ['TORCH_LOGS'] = "+guards,+perf_hints"
# os.environ['TORCHDYNAMO_VERBOSE'] = '1'

# %% ../nbs/Model inference tests-Copy1.ipynb 2
import torch
import pylab as plt
from fastprogress import progress_bar
import time

from torch.profiler import profile, record_function, ProfilerActivity

# %% ../nbs/Model inference tests-Copy1.ipynb 4
from whisperspeech.pipeline import Pipeline

# %% ../nbs/Model inference tests-Copy1.ipynb 5
import html
from IPython.display import display, HTML

def trace_handler(p):
    table = p.key_averages().table(sort_by="self_cpu_time_total", row_limit=10)
    display(HTML(f'<pre style="white-space: pre; font-size: 80%;">{html.escape(table)}</pre>'))
    fname = f"trace-s2a-{p.step_num}.json"
    p.export_chrome_trace(fname)
    display(HTML(f'<a href="{fname}" download>Download {fname}</a>'))

# %% ../nbs/Model inference tests-Copy1.ipynb 7
pipe = Pipeline(t2s_ref='t2s-small-en+pl.model', s2a_ref='s2a-q4-tiny-en+pl.model')

# for emb in pipe.s2a.embds.embeddings:
#     emb.convert_for_eval()

# for l in pipe.s2a.decoder.layers:
#     l.setup_kv_cache(1, 2250, 750)
    
# pipe.t2s.optimize()
# pipe.s2a.optimize()
    
# for l in pipe.s2a.decoder.layers:
#     l.attn.convert_for_eval()
#     l.cross_attn.convert_for_eval()
# for l in pipe.s2a.encoder:
#     l.attn.convert_for_eval()

# for l in pipe.t2s.decoder.layers:
#     l.attn.convert_for_eval()
#     l.cross_attn.convert_for_eval()
# for l in pipe.t2s.encoder.layers:
#     l.attn.convert_for_eval()
    
# torch._dynamo.reset()
# pipe.t2s.encoder = torch.compile(pipe.t2s.encoder, mode="reduce-overhead", fullgraph=True)
# pipe.t2s.decoder = torch.compile(pipe.t2s.decoder, mode="reduce-overhead", fullgraph=True)
# pipe.s2a.decoder = torch.compile(pipe.s2a.decoder, mode="reduce-overhead", fullgraph=True)
# pipe.s2a.prefill = torch.compile(pipe.s2a.prefill, mode="reduce-overhead", fullgraph=True)

# pipe.t2s.encoder = torch.compile(pipe.t2s.encoder, mode="reduce-overhead", fullgraph=True)
# pipe.s2a._encoder = torch.compile(pipe.s2a._encoder, fullgraph=True)

# pipe.s2a.generate_next = torch.compile(pipe.s2a.generate_next, mode="max-autotune", fullgraph=True)
# pipe.t2s.generate_next = torch.compile(pipe.t2s.generate_next, mode="max-autotune", fullgraph=True)

# %% ../nbs/Model inference tests-Copy1.ipynb 38
start = time.time()
pipe.vocoder.decode_to_notebook(pipe.generate_atoks("Welcome to Burger Shack! What can I get for you today?"))
print(time.time() - start)

# %% ../nbs/Model inference tests-Copy1.ipynb 39
pipe.vocoder.decode_to_file('test-output.mp3', pipe.generate_atoks("Welcome to Burger Shack!"))
