config = {
    'initializer_gain': 1.0,
    'hidden_size': 512,
    'num_hidden_layers': 6,
    'num_heads': 8,
    'filter_size': 2048,
    'layer_postprocess_dropout': 0.1,
    'attention_dropout': 0.1,
    'relu_dropout': 0.1,
    'allow_ffn_pad': True,
    'norm_before': False,
    'activation': 'relu',
}
