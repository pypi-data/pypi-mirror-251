Metadata-Version: 2.1
Name: clipsai
Version: 0.2.1
Summary: Clips AI is an open-source Python library that automatically converts long videos into clips
Home-page: https://clipsai.com/
Author: Benjamin Smidt, Johann Ramirez, Armel Talla
Author-email: support@clipsai.com
License: MIT
Project-URL: Documentation, https://docs.clipsai.com/
Project-URL: Homepage, https://clipsai.com/
Project-URL: Repository, https://github.com/ClipsAI/clipsai
Project-URL: Issues, https://github.com/ClipsAI/clipsai/issues
Classifier: Programming Language :: Python :: 3
Classifier: License :: OSI Approved :: MIT License
Classifier: Operating System :: OS Independent
Requires-Python: >=3.9
Description-Content-Type: text/markdown
License-File: LICENSE
Requires-Dist: av
Requires-Dist: facenet-pytorch
Requires-Dist: matplotlib
Requires-Dist: mediapipe
Requires-Dist: nltk
Requires-Dist: numpy
Requires-Dist: opencv-python
Requires-Dist: pandas
Requires-Dist: psutil
Requires-Dist: pyannote.audio
Requires-Dist: pyannote.core
Requires-Dist: pynvml
Requires-Dist: pytest
Requires-Dist: python-magic
Requires-Dist: scenedetect
Requires-Dist: scikit-learn
Requires-Dist: sentence-transformers
Requires-Dist: scipy
Requires-Dist: torch
Provides-Extra: dev
Requires-Dist: black ; extra == 'dev'
Requires-Dist: black[jupyter] ; extra == 'dev'
Requires-Dist: build ; extra == 'dev'
Requires-Dist: flake8 ; extra == 'dev'
Requires-Dist: ipykernel ; extra == 'dev'
Requires-Dist: pytest ; extra == 'dev'
Requires-Dist: twine ; extra == 'dev'

# ClipsAI

<!-- [![PyPI version](https://badge.fury.io/py/project-name.svg)](https://badge.fury.io/py/project-name) -->
[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)


## Quickstart

Clips AI is an open-source Python library that automatically converts long videos into
clips. With just a few lines of code, you can segment a video into multiple clips and
resize its aspect ratio from 16:9 to 9:16.

> **Note:** Clips AI is designed for audio-centric, narrative-based videos such as
podcasts, interviews, speeches, and sermons. It actively employs video transcripts to
identify and create clips. Our resizing algorithm dynamically reframes and focuses on
the current speaker, converting the video into various aspect ratios.

For full documentation, visit [Clips AI Documentation](https://clipsai.com).
Check out a [UI demo](https://demo.clipsai.com) with clips generated by this library.

### Installation

```bash
pip install clipsai
```

```bash
pip install whisperx@git+https://github.com/m-bain/whisperx.git
```

## Creating clips

Clips are created based on the transcript of a video. You'll first need to transcribe
the video, and then you can create clips based on the transcript.
[WhisperX](https://github.com/m-bain/whisperX) is utilized under the hood to transcribe
videos.

```python
from clipsai import clip, transcribe

transcription = transcribe("video.mp4")
clips = clip(transcription)

print("StartTime: ", clips[0].start_time)
print("EndTime: ", clips[0].end_time)
```

## Resizing a video

You'll need to create an access token on hugging face to resize a video because 
[Pyannote](https://github.com/pyannote/pyannote-audio) is utilized for speaker 
diarization. You won't be charged for using Pyannote and instructions are on the
[Pyannote HuggingFace ](https://huggingface.co/pyannote/speaker-diarization-3.0) page.

```python
from clipsai import resize

crops = resize(
    video_file_path="video.mp4",
    pyannote_auth_token="pyannote_token",
    aspect_ratio=(9, 16)
)

print("Crops: ", crops.segments)
```
