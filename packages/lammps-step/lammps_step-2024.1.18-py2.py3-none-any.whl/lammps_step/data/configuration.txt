[lammps-step]
# How many cores to use for LAMMPS.  Set to 1 to use only the serial
# version of LAMMPS. LAMMPS will try to choose a reasonable number of
# cores based on the size of the system and other parameters. The
# option 'lammps-atoms-per-core' can be used to tune this estimate.
#
# The default is 'available', meaning use all the cores available to
# the calculation if that makes sense based on the type of
# calculation. Otherwise you can give a number here. Note that the
# [DEFAULT] section may override the default.

# ncores = available

# The optimal number of atoms per core. You may wish to change this if
# you consistently use expensive potentials, or have e.g. GPUs. The
# default is 1000.

# lammps-atoms-per-core = 1000

# Whether to write local HTML files for the graphs, etc. generated so
# they can be viewed without the SEAMM Dashboard.

# html = False

# Information about where/how the executables are installed
# installation may be 'path', 'conda' or 'modules'. If a module is
# specified it will be loaded and those executables used.  In this
# case, any path specified using lammps-path will be ignored.

installation = not installed
conda-environment =
modules = 
gpu-modules =

# The path to the executables. Can be empty or not present, in which
# case the default PATH is used.  If a path is given, lmp_serial and
# lmp_mpi from this location will be used. If mpiexec is also present
# it will be used; otherwise mpiexec from the normal PATH will be
# used. If mpiexec or lmp_mpi is not found, only the serial version of
# LAMMPS will be used. Conversely, if lmp_serial is not present,
# lmp_mpi will always be used, though possible on just one core for
# smaller calculations.
#
# Ignored if a module is used. The default is to use the PATH
# environment variable.

lammps-path =

# The various LAMMPS executables. You should leave these as their default
# values unless you have a special need. These values are combined with
# lammps-path to get the location of the executables.
#
# If you are running from a queueing system such as SLURM, if MPI was compiled
# to know about the batch system, you should be able to use something like
# 'srun' or 'mpirun' with no further arguments to run parallel tasks. If
# you are not lucky, and need the parameters, you can add them to the command
# line like this:
#
# mpiexec = mpirun -n {NTASKS} -H {NODELIST} -npernode {NTASKS_PER_NODE}
#
# SEAMM picks up the environment variables such as SLURM_NTASKS, strips the
# prefix from them and replaces any instances in the command line that are
# enclosed in braces. Hopefully this lets you construct the correct command
# line.
#
# The LAMMPS installer does not handle these variables! They are here
# just in case!

# lammps-serial = lmp_serial
# lammps-mpi = lmp_mpi
# mpiexec = mpiexec

# Optional command-line arguments for LAMMPS. These are usually used with accelerators
# such as Kokkos or GPU to automatically apply the accelerator. Again there is a normal
# and GPU version

# cmd-args = -k on -sf kk
# gpu-cmd-args = -k on g {ngpus} -sf kk -pk kokkos gpu/aware on newton on neigh full neigh/thread on neigh/transpose off comm device

