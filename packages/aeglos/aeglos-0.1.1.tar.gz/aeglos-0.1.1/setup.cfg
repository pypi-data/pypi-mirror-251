[metadata]
name = aeglos
version = 0.1.1
author = Alex, Anirudh
author_email = alexandru@aeglos.ai, anirudh@aeglos.ai
description = This package adds a SDK that guardrails your LLM applications from malicious prompts
long_description = file: README.md
long_description_content_type = text/markdown
url = https://aeglos.ai/
project_urls = 
	Bug Tracker = https://github.com/anirudhramoo/prompInjection/issues
classifiers = 
	Programming Language :: Python :: 3
	License :: OSI Approved :: MIT License
	Operating System :: OS Independent

[options]
package_dir = 
	= src
packages = find:
python_requires = >=3.6

[options.packages.find]
where = src

[egg_info]
tag_build = 
tag_date = 0

