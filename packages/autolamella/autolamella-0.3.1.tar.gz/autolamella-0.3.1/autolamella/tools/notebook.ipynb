{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import data\n",
    "import plotly.express as px\n",
    "import pandas   as pd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = r\"C:\\Users\\lnae0002\\Desktop\\autolamella\\autolamella\\log\\3\"\n",
    "\n",
    "df_sample, df_history, df_shift, df_steps  = data.calculate_statistics_dataframe(path)\n",
    "display(df_shift)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "points = pd.DataFrame(df_shift[\"shift\"].tolist(), columns=[\"x\", \"y\"])\n",
    "points[\"lamella\"] = df_shift[\"lamella\"]\n",
    "fig = px.scatter(points, x=\"x\", y='y', color=\"lamella\")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(df_steps)\n",
    "fig = px.scatter(df_steps, x=\"step_n\", y=\"timestamp\", color=\"stage\", symbol='lamella')\n",
    "fig.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate difference in timestamp between rows\n",
    "df_steps['delta'] = df_steps['timestamp'].diff()\n",
    "\n",
    "display(df_steps)\n",
    "\n",
    "px.bar(df_steps, x=\"lamella\", y=\"delta\", color=\"step\", facet_col=\"stage\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.bar(df_history, x=\"stage\", y=\"duration\", color=\"petname\", barmode=\"group\")\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# display(df_history)\n",
    "df_history[\"duration\"].mean()\n",
    "\n",
    "df_sorted = df_history.groupby([\"stage\", \"petname\"]).mean()\n",
    "df_sorted.drop(columns=[\"start\", \"end\"], inplace=True)\n",
    "df_sorted.reset_index(inplace=True)\n",
    "display(df_sorted)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "points = pd.DataFrame(df_sample[\"lamella.centre\"].tolist(), columns=[\"x\", \"y\"])\n",
    "points[\"petname\"] = df_history[\"petname\"].unique()\n",
    "display(points)\n",
    "fig = px.scatter(points, x=\"x\", y='y', color=\"petname\")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sample[\"petname\"] = df_history[\"petname\"].unique()\n",
    "fig = px.scatter_3d(df_sample, x=\"lamella.x\", y='lamella.y', z='lamella.z', color=\"petname\")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SpaceTOMO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from PIL import Image\n",
    "import tifffile as tff\n",
    "import glob\n",
    "\n",
    "\n",
    "\n",
    "PATH = \"../../../data/spacetomo/imagesTr/\"\n",
    "\n",
    "\n",
    "filenames = glob.glob(PATH + \"*.png\")\n",
    "\n",
    "from pprint import pprint\n",
    "\n",
    "# pprint(filenames)\n",
    "\n",
    "# convert to tiff, save in separate dir   \n",
    "for filename in filenames:\n",
    "    im = Image.open(filename)\n",
    "    im.save(filename.replace(\".png\", \".tif\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Detection Tools\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import os\n",
    "import glob\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "\n",
    "path = \"../../../data/autolamella-paper/model-development/train/serial-liftout/test/\"\n",
    "\n",
    "filenames = sorted(glob.glob(os.path.join(path, \"*.tif*\")))\n",
    "\n",
    "\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(os.path.join(path, \"data.csv\"))\n",
    "\n",
    "display(df.head())\n",
    "\n",
    "filenames = []\n",
    "for fname in df[\"filename\"].values:\n",
    "    filenames += glob.glob(os.path.join(path, f\"*{fname}*\"))\n",
    "\n",
    "print(len(filenames))\n",
    "\n",
    "from pprint import pprint\n",
    "pprint(filenames)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(os.path.join(path, \"data.csv\"))\n",
    "\n",
    "\n",
    "fname = df[\"filename\"].values[0]\n",
    "\n",
    "df_filt = df[df[\"filename\"] == fname]\n",
    "\n",
    "\n",
    "display(df)\n",
    "\n",
    "# drop rows with filename = fname\n",
    "df.drop(df[df[\"filename\"] == fname].index, inplace=True)\n",
    "\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "PATH = \"/home/patrick/github/data/autolamella-paper/model-development/train/summary.csv\"\n",
    "\n",
    "df = pd.read_csv(PATH)\n",
    "# group by metho, dataset, -> count\n",
    "\n",
    "df_grouped = df.groupby([\"method\", \"dataset\"]).count()\n",
    "\n",
    "display(df_grouped)\n",
    "\n",
    "# group by method\n",
    "df_grouped = df.groupby([\"method\"]).count()\n",
    "\n",
    "display(df_grouped)\n",
    "\n",
    "# group by dataset\n",
    "df_grouped = df.groupby([\"dataset\"]).count()\n",
    "\n",
    "display(df_grouped)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HF Dataset\n",
    "\n",
    "https://github.com/huggingface/transformers/tree/main/examples/pytorch/semantic-segmentation#note-on-custom-data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset, DatasetDict, Image\n",
    "import os\n",
    "import glob\n",
    "\n",
    "\n",
    "TRAIN_PATH = \"/home/patrick/github/data/autolamella-paper/model-development/train/autolamella-waffle/train/\"\n",
    "TEST_PATH = \"/home/patrick/github/data/autolamella-paper/model-development/train/autolamella-waffle/test/\"\n",
    "\n",
    "\n",
    "# your images can of course have a different extension\n",
    "image_paths_train = sorted(glob.glob(os.path.join(TRAIN_PATH, \"*.tif*\")))\n",
    "label_paths_train = sorted(glob.glob(os.path.join(TRAIN_PATH, \"labels\", \"*.tif*\")))\n",
    "\n",
    "# same for test\n",
    "image_paths_test = sorted(glob.glob(os.path.join(TEST_PATH, \"*.tif*\")))\n",
    "label_paths_test = sorted(glob.glob(os.path.join(TEST_PATH, \"labels\", \"*.tif*\")))\n",
    "\n",
    "def create_dataset(image_paths, label_paths):\n",
    "    dataset = Dataset.from_dict({\"image\": sorted(image_paths),\n",
    "                                \"label\": sorted(label_paths)})\n",
    "    dataset = dataset.cast_column(\"image\", Image())\n",
    "    dataset = dataset.cast_column(\"label\", Image())\n",
    "\n",
    "    return dataset\n",
    "\n",
    "# step 1: create Dataset objects\n",
    "train_dataset = create_dataset(image_paths_train, label_paths_train)\n",
    "test_dataset = create_dataset(image_paths_test, label_paths_test)\n",
    "\n",
    "# step 2: create DatasetDict\n",
    "dataset = DatasetDict({\n",
    "    \"train\": train_dataset,\n",
    "    \"test\": test_dataset,\n",
    "  }\n",
    ")\n",
    "\n",
    "display(dataset)\n",
    "\n",
    "\n",
    "# step 3: push to hub (assumes you have ran the huggingface-cli login command in a terminal/notebook)\n",
    "# dataset.push_to_hub(\"name of repo on the hub\")\n",
    "\n",
    "# optionally, you can push to a private repo on the hub\n",
    "dataset.push_to_hub(\"patrickcleeve/autolamella\", private=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset, DatasetDict, Image, NamedSplit, DatasetInfo, ClassLabel, Features\n",
    "import os\n",
    "import glob\n",
    "\n",
    "\n",
    "WAFFLE_TRAIN_PATH = \"/home/patrick/github/data/autolamella-paper/model-development/train/autolamella-waffle/train/\"\n",
    "WAFFLE_TEST_PATH = \"/home/patrick/github/data/autolamella-paper/model-development/train/autolamella-waffle/test/\"\n",
    "\n",
    "AUTOLIFTOUT_TRAIN_PATH = \"/home/patrick/github/data/autolamella-paper/model-development/train/autoliftout/train/\"\n",
    "AUTOLIFTOUT_TEST_PATH = \"/home/patrick/github/data/autolamella-paper/model-development/train/autoliftout/test/\"\n",
    "\n",
    "SERIAL_LIFTOUT_TRAIN_PATH = \"/home/patrick/github/data/autolamella-paper/model-development/train/serial-liftout/train/\"\n",
    "SERIAL_LIFTOUT_TEST_PATH = \"/home/patrick/github/data/autolamella-paper/model-development/train/serial-liftout/test/\"\n",
    "\n",
    "\n",
    "\n",
    "def create_dataset(image_paths, label_paths, split: str, info=None):\n",
    "\n",
    "    info = DatasetInfo(\n",
    "        description=\"Autolamella Dataset\",\n",
    "        homepage= \"https://github.com/DeMarcoLab/autolamella\",\n",
    "        license= \"MIT\",\n",
    "        # features= Features({\n",
    "        #     # \"image\": Image(),\n",
    "        #     # \"annotation\": Image(),\n",
    "        #     \"label\": ClassLabel(\n",
    "        #         num_classes=6,\n",
    "        #         names=[\"background\", \"lamella\", \"manipulator\", \"landing_post\", \"copper_adaptor\", \"volume_block\"]),\n",
    "        # }),\n",
    "    )\n",
    "    # TODO: add features to the dataset\n",
    "\n",
    "    dataset = Dataset.from_dict({\"image\": sorted(image_paths),\n",
    "                                \"annotation\": sorted(label_paths),\n",
    "                                # \"split\": [split] * len(image_paths)\n",
    "                                # \"features\": [info.features] * len(image_paths),\n",
    "                                },\n",
    "                                split=split,\n",
    "                                # split=split,\n",
    "                                # features=info.features, \n",
    "                                info=info\n",
    "                                )\n",
    "    dataset = dataset.cast_column(\"image\", Image())\n",
    "    dataset = dataset.cast_column(\"annotation\", Image())\n",
    "\n",
    "\n",
    "    return dataset\n",
    "\n",
    "train_paths = [WAFFLE_TRAIN_PATH, AUTOLIFTOUT_TRAIN_PATH, SERIAL_LIFTOUT_TRAIN_PATH]\n",
    "test_paths = [WAFFLE_TEST_PATH, AUTOLIFTOUT_TEST_PATH, SERIAL_LIFTOUT_TEST_PATH]\n",
    "\n",
    "\n",
    "autolamella_datasets = {\n",
    "    \"waffle\": {\"train\": {}, \"test\": {}},\n",
    "    \"liftout\": {\"train\": {}, \"test\": {}},\n",
    "    \"serial-liftout\": {\"train\": {}, \"test\": {}},\n",
    "}\n",
    "\n",
    "for name, train_path, test_path in zip(autolamella_datasets.keys(), train_paths, test_paths):\n",
    "    # your images can of course have a different extension\n",
    "    image_paths_train = sorted(glob.glob(os.path.join(train_path, \"*.tif*\")))\n",
    "    label_paths_train = sorted(glob.glob(os.path.join(train_path, \"labels\", \"*.tif*\")))\n",
    "\n",
    "    # same for test\n",
    "    image_paths_test = sorted(glob.glob(os.path.join(test_path, \"*.tif*\")))\n",
    "    label_paths_test = sorted(glob.glob(os.path.join(test_path, \"labels\", \"*.tif*\")))\n",
    "\n",
    "    # step 1: create Dataset objects\n",
    "    train_dataset = create_dataset(image_paths_train, label_paths_train, \"train\")\n",
    "    test_dataset = create_dataset(image_paths_test, label_paths_test, \"test\")\n",
    "\n",
    "    autolamella_datasets[name][\"train\"] = train_dataset\n",
    "    autolamella_datasets[name][\"test\"] = test_dataset\n",
    "\n",
    "\n",
    "# create DatasetDict\n",
    "waffle_dataset = DatasetDict(autolamella_datasets[\"waffle\"])\n",
    "liftout_dataset = DatasetDict(autolamella_datasets[\"liftout\"])\n",
    "serial_liftout_dataset = DatasetDict(autolamella_datasets[\"serial-liftout\"])\n",
    "\n",
    "\n",
    "# https://huggingface.co/docs/datasets/v2.16.1/en/repository_structure\n",
    "\n",
    "\n",
    "\n",
    "print(waffle_dataset)\n",
    "print(liftout_dataset)\n",
    "print(serial_liftout_dataset)\n",
    "\n",
    "\n",
    "# optionally, you can push to a private repo on the hub\n",
    "waffle_dataset.push_to_hub(\"patrickcleeve/autolamella\", config_name=\"waffle\", private=True)\n",
    "liftout_dataset.push_to_hub(\"patrickcleeve/autolamella\", config_name=\"liftout\",private=True)\n",
    "serial_liftout_dataset.push_to_hub(\"patrickcleeve/autolamella\", config_name=\"serial-liftout\",private=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# info, features\n",
    "\n",
    "info = DatasetInfo.from_dict({\n",
    "    \"description\": \"Autolamella dataset\",\n",
    "    \"homepage\": \"https://github.com/DeMarcoLab/autolamella\",\n",
    "    \"license\": \"MIT\",\n",
    "    \"features\": Features({\n",
    "        \"label\": ClassLabel(num_classes=6, \n",
    "                            names=[\"background\", \"lamella\", \"manipulator\", \"landing_post\", \"copper_adaptor\", \"volume_block\"])\n",
    "    }),\n",
    "})\n",
    "\n",
    "from pprint import pprint\n",
    "pprint(info.features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "ds = load_dataset(\"patrickcleeve/autolamella\", name=\"waffle\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from fibsem.segmentation.utils import decode_segmap_v2\n",
    "\n",
    "# random data\n",
    "idx = random.randint(0, len(ds[\"train\"]))\n",
    "image = np.asarray(ds[\"train\"][idx][\"image\"])\n",
    "mask = np.asarray(ds[\"train\"][idx][\"annotation\"])\n",
    "\n",
    "# metadata\n",
    "split = ds[\"train\"].split\n",
    "config_name = ds[\"train\"].config_name\n",
    "\n",
    "plt.title(f\"{config_name}-{split}-{idx:02d}\")\n",
    "plt.imshow(image, cmap=\"gray\", alpha=0.7)\n",
    "plt.imshow(decode_segmap_v2(mask), alpha=0.3)\n",
    "plt.axis(\"off\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "images = [np.asarray(img) for img in ds[\"train\"][\"image\"]]\n",
    "masks = [np.asarray(mask) for mask in ds[\"train\"][\"annotation\"]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Validate Dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import os\n",
    "import glob\n",
    "import pandas as pd\n",
    "\n",
    "PATH = \"/home/patrick/github/data/autolamella-paper/model-development/train/\"\n",
    "\n",
    "methods = [\"autolamella-waffle\", \"autoliftout\", \"serial-liftout\"]\n",
    "splits = [\"train\", \"test\"]\n",
    "\n",
    "df = pd.read_csv(os.path.join(PATH, \"summary2.csv\"))\n",
    "\n",
    "display(df.head())\n",
    "\n",
    "print(len(df))\n",
    "\n",
    "# loop over methods, splits, check that each file exists\n",
    "\n",
    "to_drop = []\n",
    "for method in methods:\n",
    "    for split in splits:\n",
    "        df_filt = df[(df[\"method\"] == method) & (df[\"dataset\"] == split)]\n",
    "        print(f\"Method: {method}, Split: {split}, Num files: {len(df_filt)}\")\n",
    "        for fname in df_filt[\"filename\"].values:\n",
    "            if not os.path.exists(os.path.join(PATH, method, split, fname)):\n",
    "                print(f\"File {fname} does not exist\")\n",
    "\n",
    "                to_drop.append((fname, split))\n",
    "\n",
    "# group by method, dataset, -> count\n",
    "\n",
    "# drop rows with filename = fname\n",
    "\n",
    "# show rows in to_drop\n",
    "\n",
    "# drop rows in to_drop\n",
    "for fname, split in to_drop:\n",
    "    df.drop(df[(df[\"filename\"] == fname) & (df[\"dataset\"] == split)].index, inplace=True)\n",
    "\n",
    "    \n",
    "\n",
    "df_grouped = df.groupby([\"method\", \"dataset\"]).count()\n",
    "display(df_grouped)\n",
    "print(len(df))\n",
    "\n",
    "# save to csv\n",
    "# df.to_csv(os.path.join(PATH, \"summary2.csv\"), index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "ds = load_dataset(\"patrickcleeve/autolamella\", name=\"waffle\")\n",
    "print(ds)\n",
    "ds = load_dataset(\"patrickcleeve/autolamella\", name=\"liftout\")\n",
    "print(ds)\n",
    "ds = load_dataset(\"patrickcleeve/autolamella\", name=\"serial-liftout\")\n",
    "print(ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from fibsem.segmentation.utils import decode_segmap_v2\n",
    "import tifffile as tff\n",
    "import random\n",
    "\n",
    "\n",
    "df = pd.read_csv(os.path.join(PATH, \"summary.csv\"))\n",
    "\n",
    "split = \"train\"\n",
    "method = \"waffle\"\n",
    "\n",
    "df_filt = df[(df[\"method\"] == method) & (df[\"split\"] == split)]\n",
    "print(f\"Method: {method}, Split: {split}, Num files: {len(df_filt)}\")\n",
    "\n",
    "\n",
    "idx = random.randint(0, len(df_filt))\n",
    "fname = df_filt[\"filename\"].values[idx]\n",
    "print(idx, fname)\n",
    "\n",
    "ds = load_dataset(\"patrickcleeve/autolamella\", name=method, split=split)\n",
    "\n",
    "image = tff.imread(os.path.join(PATH, method, split, fname))\n",
    "mask = tff.imread(os.path.join(PATH, method, split, \"labels\", fname))\n",
    "\n",
    "image = np.asarray(ds[idx][\"image\"])\n",
    "mask = np.asarray(ds[idx][\"annotation\"])\n",
    "\n",
    "fig, ax = plt.subplots(1, 2, figsize=(10, 5))\n",
    "\n",
    "fig.suptitle(f\"{method}-{split}-{idx:02d}\")\n",
    "ax[0].imshow(image, cmap=\"gray\", alpha=0.7)\n",
    "ax[0].imshow(decode_segmap_v2(mask), alpha=0.3)\n",
    "ax[0].axis(\"off\")\n",
    "ax[1].imshow(image, cmap=\"gray\", alpha=0.7)\n",
    "ax[1].imshow(decode_segmap_v2(mask), alpha=0.3)\n",
    "ax[1].axis(\"off\")\n",
    "plt.show()\n",
    "\n",
    "print(df_filt.values[idx])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combine multiple datasets\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset, concatenate_datasets\n",
    "waffle_train_ds = load_dataset(\"patrickcleeve/autolamella\", name=\"waffle\", split=\"train\")\n",
    "liftout_train_ds = load_dataset(\"patrickcleeve/autolamella\", name=\"liftout\", split=\"train\")\n",
    "serial_liftout_train_ds = load_dataset(\"patrickcleeve/autolamella\", name=\"serial-liftout\", split=\"train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset, concatenate_datasets\n",
    "\n",
    "# load invidual datasets\n",
    "waffle_train_ds = load_dataset(\"patrickcleeve/autolamella\", name=\"waffle\", split=\"train\")\n",
    "liftout_train_ds = load_dataset(\"patrickcleeve/autolamella\", name=\"liftout\", split=\"train\")\n",
    "serial_liftout_train_ds = load_dataset(\"patrickcleeve/autolamella\", name=\"serial-liftout\", split=\"train\")\n",
    "\n",
    "# concatenate datasets (e.g. mega model)\n",
    "train_ds = concatenate_datasets([waffle_train_ds, liftout_train_ds, serial_liftout_train_ds])\n",
    "\n",
    "print(train_ds)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Volume Block Corners\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from datasets import load_dataset, concatenate_datasets\n",
    "import matplotlib.pyplot as plt \n",
    "import numpy as np\n",
    "from fibsem.segmentation.utils import decode_segmap_v2\n",
    "\n",
    "ds = load_dataset(\"patrickcleeve/autolamella\", name=\"serial-liftout\", split=\"train\")\n",
    "\n",
    "\n",
    "image = np.asarray(ds[4][\"image\"])\n",
    "mask = np.asarray(ds[4][\"annotation\"])\n",
    "\n",
    "\n",
    "from fibsem.detection.detection import VolumeBlockTopLeftCorner, VolumeBlockTopRightCorner, VolumeBlockBottomLeftCorner, VolumeBlockBottomRightCorner\n",
    "\n",
    "top_left_corner = VolumeBlockTopLeftCorner().detect(None, mask)\n",
    "top_right_corner = VolumeBlockTopRightCorner().detect(None, mask)\n",
    "bottom_left_corner = VolumeBlockBottomLeftCorner().detect(None, mask)\n",
    "bottom_right_corner = VolumeBlockBottomRightCorner().detect(None, mask)\n",
    "\n",
    "\n",
    "plt.imshow(image, cmap=\"gray\", alpha=0.7)\n",
    "plt.imshow(decode_segmap_v2(mask), alpha=0.3)\n",
    "plt.scatter(top_left_corner[0], top_left_corner[1], color=\"magenta\")\n",
    "plt.scatter(top_right_corner[0], top_right_corner[1], color=\"magenta\")\n",
    "plt.scatter(bottom_left_corner[0], bottom_left_corner[1], color=\"magenta\")\n",
    "plt.scatter(bottom_right_corner[0], bottom_right_corner[1], color=\"magenta\")\n",
    "plt.axis(\"off\")\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from fibsem import utils\n",
    "from fibsem.patterning import _get_milling_stages\n",
    "from fibsem.ui.utils import _draw_milling_stages_on_image \n",
    "from fibsem.structures import Point\n",
    "from copy import deepcopy\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "microscope, settings = utils.setup_session()\n",
    "\n",
    "protocol = utils.load_protocol(\"protocol.yaml\")\n",
    "settings.protocol = protocol\n",
    "\n",
    "# get evenly spaced points\n",
    "width = 100e-6\n",
    "n_patterns = 4\n",
    "pos_x = np.linspace(-width/2, width/2, n_patterns)\n",
    "points = [Point(x, 0) for x in pos_x]\n",
    "\n",
    "# acquire image for visualiation\n",
    "settings.image.hfw = 150e-6\n",
    "image = microscope.acquire_image(settings.image)\n",
    "\n",
    "block_stages = []\n",
    "for i, pt in enumerate(points):\n",
    "\n",
    "    # get milling stage\n",
    "    stage = _get_milling_stages(\"copper-block-removal\", deepcopy(settings.protocol), deepcopy(pt))[0]\n",
    "    stage.name = f\"Copper-Block-Removal-{i:02d}\"\n",
    "    block_stages.append(deepcopy(stage))\n",
    "\n",
    "# get top removal stage\n",
    "top_stages= _get_milling_stages(\"copper-block-removal-top\", settings.protocol, Point(0, 10e-6))\n",
    "\n",
    "stages = top_stages + block_stages\n",
    "\n",
    "# draw stages on image\n",
    "fig = _draw_milling_stages_on_image(image, stages)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Apply Microscope Configuration\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "\n",
    "from fibsem import utils\n",
    "from pprint import pprint\n",
    "\n",
    "microscope, settings = utils.setup_session()\n",
    "\n",
    "# apply configuration\n",
    "utils.apply_configuration(microscope, settings)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fibsem.structures import BeamType\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# # beam system settings\n",
    "# electron_system_settings = microscope.get_beam_system_settings(BeamType.ELECTRON)\n",
    "# ion_system_settings = microscope.get_beam_system_settings(BeamType.ION)\n",
    "\n",
    "# pprint(electron_system_settings.__to_dict__())\n",
    "# pprint(ion_system_settings.__to_dict__())\n",
    "\n",
    "# electron_settings = microscope.get_beam_settings(BeamType.ELECTRON)\n",
    "# ion_settings = microscope.get_beam_settings(BeamType.ION)\n",
    "\n",
    "# pprint(electron_settings.__to_dict__())\n",
    "# pprint(ion_settings.__to_dict__())\n",
    "\n",
    "\n",
    "# # detector settings\n",
    "# electron_detector_settings = microscope.get_detector_settings(BeamType.ELECTRON)\n",
    "# ion_detector_settings = microscope.get_detector_settings(BeamType.ION)\n",
    "\n",
    "# pprint(electron_detector_settings.__to_dict__())\n",
    "# pprint(ion_detector_settings.__to_dict__())\n",
    "\n",
    "# check that microscope and settings are the same\n",
    "assert microscope.hardware_settings == settings.hardware\n",
    "assert microscope.stage_settings == settings.system.stage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Microscope Fingerprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "microscope, settings = utils.setup_session()\n",
    "\n",
    "electron_system_settings = microscope.get_beam_system_settings(BeamType.ELECTRON)\n",
    "ion_system_settings = microscope.get_beam_system_settings(BeamType.ION)\n",
    "imaging_settings = microscope.get_imaging_settings(BeamType.ELECTRON)\n",
    "\n",
    "\n",
    "pprint(electron_system_settings.__to_dict__())\n",
    "pprint(ion_system_settings.__to_dict__())\n",
    "\n",
    "pprint(imaging_settings.__to_dict__())\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pprint(microscope.system.__to_dict__())\n",
    "\n",
    "\n",
    "\n",
    "# check beams are on\n",
    "\n",
    "# check chamber is pumped\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Default configuration: default-configuration\n",
      "Default configuration path: /home/patrick/github/fibsem/fibsem/config/microscope-configuration.yaml\n",
      "2024-01-17 00:40:23,601 — root — INFO — connect_to_microscope:5231 — Connected to Demo Microscope\n",
      "2024-01-17 00:40:23,601 — root — INFO — connect_to_microscope:5232 — Microscope client connected to model Demo with serial number 123456 and software version 0.1\n",
      "2024-01-17 00:40:23,602 — root — INFO — reset_beam_shifts:5339 — Resetting beam shifts\n",
      "2024-01-17 00:40:23,602 — root — INFO — setup_session:221 — Finished setup for session: demo_2024-01-17-12-40-23AM\n",
      "False\n",
      "False\n",
      "Chamber State:  Pumped\n",
      "Chamber Pressure:  1e-06\n",
      "2024-01-17 00:40:23,602 — root — INFO — set:5860 — Setting vent to True (None)\n",
      "Chamber State:  Vented\n",
      "Chamber Pressure:  100000.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/patrick/miniconda3/envs/fibsem/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-01-17 00:40:25,479 — root — WARNING — get:5856 — Unknown key: stage_homed (None)\n",
      "2024-01-17 00:40:25,480 — root — WARNING — get:5856 — Unknown key: stage_linked (None)\n",
      "2024-01-17 00:40:25,480 — root — WARNING — get:5856 — Unknown key: manipulator_state (None)\n",
      "2024-01-17 00:40:25,481 — root — WARNING — validate_microscope:335 — Microscope Validation Warnings: ['Electron beam is off', 'Ion beam is off', 'Chamber is not pumped', 'Stage is not homed', 'Stage is not linked', 'Needle is not retracted']\n",
      "['Electron beam is off', 'Ion beam is off', 'Chamber is not pumped', 'Stage is not homed', 'Stage is not linked', 'Needle is not retracted']\n"
     ]
    }
   ],
   "source": [
    "# validate system\n",
    "\n",
    "from fibsem import utils\n",
    "from fibsem.structures import BeamType\n",
    "from fibsem.microscope import FibsemMicroscope\n",
    "import logging\n",
    "\n",
    "microscope, settings = utils.setup_session()\n",
    "\n",
    "print(microscope.get(\"on\", BeamType.ELECTRON))\n",
    "print(microscope.get(\"on\", BeamType.ION))\n",
    "\n",
    "# microscope.set(\"on\", True, BeamType.ELECTRON)\n",
    "# microscope.set(\"on\", True, BeamType.ION)\n",
    "\n",
    "\n",
    "# get the system is pumped\n",
    "print(\"Chamber State: \", microscope.get(\"chamber_state\"))\n",
    "print(\"Chamber Pressure: \", microscope.get(\"chamber_pressure\"))\n",
    "\n",
    "# set vent\n",
    "microscope.set(\"vent\", True)\n",
    "\n",
    "print(\"Chamber State: \", microscope.get(\"chamber_state\"))\n",
    "print(\"Chamber Pressure: \", microscope.get(\"chamber_pressure\"))\n",
    "\n",
    "from fibsem.validation import validate_microscope\n",
    "\n",
    "warnings = validate_microscope(microscope)\n",
    "\n",
    "print(warnings)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
