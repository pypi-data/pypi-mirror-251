{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Set env vars\n",
    "from operator import itemgetter\n",
    "\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain_core.runnables import RunnableLambda, RunnablePassthrough\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.callbacks import get_openai_callback\n",
    "from dotenv import load_dotenv\n",
    "from langchain.globals import set_debug\n",
    "\n",
    "set_debug(True)\n",
    "\n",
    "from contextcrunch_langchain import ConversationCruncher\n",
    "load_dotenv()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "model = ChatOpenAI()\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", \"Conversation Summary:\\n{history}\"),\n",
    "        (\"human\", \"{input}\"),\n",
    "    ]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "memory = ConversationBufferMemory()\n",
    "# memory.chat_memory.add_user_message(\"I have a ML api that i would like to monetize, so I want to create an API proxy that will handle users, authentication, signup, and billing. Can I use vercel for this?\")\n",
    "# memory.chat_memory.add_ai_message(\"Yes, you can use Vercel to create an API proxy for your machine learning API that handles user management, authentication, signup, and billing. Vercel is primarily known for its capabilities in deploying and hosting web applications and static sites, but it can also be used for serverless functions, which you can leverage to build your API proxy. Here are some key points to consider when setting up your API proxy on Vercel: Serverless Functions: You can write serverless functions in Vercel to handle various aspects of your API, like user authentication, signup, and billing. These functions can be written in languages like JavaScript, TypeScript, Python, etc. Authentication: For handling user authentication, you can integrate third-party services like Auth0, Firebase Authentication, or build your own custom solution. These services can be integrated into your serverless functions. Database Integration: You'll likely need a database to store user information and other relevant data. Vercel can connect to various databases, and you can choose one based on your requirements. Billing and Subscription Management: For billing, you can use third-party services like Stripe, PayPal, or others. These services offer APIs that you can integrate into your Vercel application to handle subscriptions, one-time payments, and other billing-related activities. API Routing and Proxying: You can set up routes in your Vercel project to manage the requests to your machine learning API. This involves receiving requests, authenticating them, and then forwarding them to your actual ML API. Environment Variables: Vercel supports environment variables, which are essential for securely storing API keys, database connection strings, and other sensitive information. Scalability: Vercel offers good scalability options, which is beneficial if your API usage grows over time. Deployment and CI/CD: Vercel provides easy deployment options and integrates with various version control systems for continuous integration and deployment. When setting up your API proxy on Vercel, you'll need to write the code for the serverless functions, set up the necessary integrations with databases and authentication/billing services, and configure your project's deployment and routing settings. Always ensure to test your setup thoroughly to handle various user scenarios and security considerations.\")\n",
    "memory.chat_memory.add_user_message(\"My favourite color is purple, my favourite food is pizza.\")\n",
    "memory.chat_memory.add_ai_message(\"I understand. Your favourite color is purple, and your favourite food is pizza.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'history': \"Human: I have a ML api that i would like to monetize, so I want to create an API proxy that will handle users, authentication, signup, and billing. Can I use vercel for this?\\nAI: Yes, you can use Vercel to create an API proxy for your machine learning API that handles user management, authentication, signup, and billing. Vercel is primarily known for its capabilities in deploying and hosting web applications and static sites, but it can also be used for serverless functions, which you can leverage to build your API proxy. Here are some key points to consider when setting up your API proxy on Vercel: Serverless Functions: You can write serverless functions in Vercel to handle various aspects of your API, like user authentication, signup, and billing. These functions can be written in languages like JavaScript, TypeScript, Python, etc. Authentication: For handling user authentication, you can integrate third-party services like Auth0, Firebase Authentication, or build your own custom solution. These services can be integrated into your serverless functions. Database Integration: You'll likely need a database to store user information and other relevant data. Vercel can connect to various databases, and you can choose one based on your requirements. Billing and Subscription Management: For billing, you can use third-party services like Stripe, PayPal, or others. These services offer APIs that you can integrate into your Vercel application to handle subscriptions, one-time payments, and other billing-related activities. API Routing and Proxying: You can set up routes in your Vercel project to manage the requests to your machine learning API. This involves receiving requests, authenticating them, and then forwarding them to your actual ML API. Environment Variables: Vercel supports environment variables, which are essential for securely storing API keys, database connection strings, and other sensitive information. Scalability: Vercel offers good scalability options, which is beneficial if your API usage grows over time. Deployment and CI/CD: Vercel provides easy deployment options and integrates with various version control systems for continuous integration and deployment. When setting up your API proxy on Vercel, you'll need to write the code for the serverless functions, set up the necessary integrations with databases and authentication/billing services, and configure your project's deployment and routing settings. Always ensure to test your setup thoroughly to handle various user scenarios and security considerations.\"}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "memory.load_memory_variables({})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using api key dc759dfe-3600-4fff-a902-b8657f65caf9\n",
      "using api key2 dc759dfe-3600-4fff-a902-b8657f65caf9\n"
     ]
    }
   ],
   "source": [
    "chain = (\n",
    "    {'history': RunnableLambda(memory.load_memory_variables) | itemgetter(\"history\"), 'input': RunnablePassthrough()}\n",
    "    | ConversationCruncher(compression_ratio=0.98)\n",
    "    | prompt\n",
    "    | model\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[1:chain:RunnableSequence] Entering Chain run with input:\n",
      "\u001b[0m{\n",
      "  \"input\": \"What is my favourite color?\"\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[1:chain:RunnableSequence > 2:chain:RunnableParallel<history,input>] Entering Chain run with input:\n",
      "\u001b[0m{\n",
      "  \"input\": \"What is my favourite color?\"\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[1:chain:RunnableSequence > 2:chain:RunnableParallel<history,input> > 3:chain:RunnablePassthrough] Entering Chain run with input:\n",
      "\u001b[0m{\n",
      "  \"input\": \"What is my favourite color?\"\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[1:chain:RunnableSequence > 2:chain:RunnableParallel<history,input> > 3:chain:RunnablePassthrough] [0ms] Exiting Chain run with output:\n",
      "\u001b[0m{\n",
      "  \"output\": \"What is my favourite color?\"\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[1:chain:RunnableSequence > 2:chain:RunnableParallel<history,input> > 4:chain:RunnableSequence] Entering Chain run with input:\n",
      "\u001b[0m{\n",
      "  \"input\": \"What is my favourite color?\"\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[1:chain:RunnableSequence > 2:chain:RunnableParallel<history,input> > 4:chain:RunnableSequence > 5:chain:load_memory_variables] Entering Chain run with input:\n",
      "\u001b[0m{\n",
      "  \"input\": \"What is my favourite color?\"\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[1:chain:RunnableSequence > 2:chain:RunnableParallel<history,input> > 4:chain:RunnableSequence > 5:chain:load_memory_variables] [1ms] Exiting Chain run with output:\n",
      "\u001b[0m{\n",
      "  \"history\": \"Human: My favourite color is purple, my favourite food is pizza.\\nAI: I understand. Your favourite color is purple, and your favourite food is pizza.\"\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[1:chain:RunnableSequence > 2:chain:RunnableParallel<history,input> > 4:chain:RunnableSequence > 6:chain:RunnableLambda] Entering Chain run with input:\n",
      "\u001b[0m{\n",
      "  \"history\": \"Human: My favourite color is purple, my favourite food is pizza.\\nAI: I understand. Your favourite color is purple, and your favourite food is pizza.\"\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[1:chain:RunnableSequence > 2:chain:RunnableParallel<history,input> > 4:chain:RunnableSequence > 6:chain:RunnableLambda] [0ms] Exiting Chain run with output:\n",
      "\u001b[0m{\n",
      "  \"output\": \"Human: My favourite color is purple, my favourite food is pizza.\\nAI: I understand. Your favourite color is purple, and your favourite food is pizza.\"\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[1:chain:RunnableSequence > 2:chain:RunnableParallel<history,input> > 4:chain:RunnableSequence] [3ms] Exiting Chain run with output:\n",
      "\u001b[0m{\n",
      "  \"output\": \"Human: My favourite color is purple, my favourite food is pizza.\\nAI: I understand. Your favourite color is purple, and your favourite food is pizza.\"\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[1:chain:RunnableSequence > 2:chain:RunnableParallel<history,input>] [10ms] Exiting Chain run with output:\n",
      "\u001b[0m{\n",
      "  \"history\": \"Human: My favourite color is purple, my favourite food is pizza.\\nAI: I understand. Your favourite color is purple, and your favourite food is pizza.\",\n",
      "  \"input\": \"What is my favourite color?\"\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[1:chain:RunnableSequence > 7:chain:call] Entering Chain run with input:\n",
      "\u001b[0m{\n",
      "  \"history\": \"Human: My favourite color is purple, my favourite food is pizza.\\nAI: I understand. Your favourite color is purple, and your favourite food is pizza.\",\n",
      "  \"input\": \"What is my favourite color?\"\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[1:chain:RunnableSequence > 7:chain:call] [960ms] Exiting Chain run with output:\n",
      "\u001b[0m{\n",
      "  \"history\": \"Human: My favourite color is purple, my favourite food is pizza.\\nAI: I understand. Your favourite color is purple, and your favourite food is pizza.\",\n",
      "  \"input\": \"What is my favourite color?\"\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[1:chain:RunnableSequence > 8:prompt:ChatPromptTemplate] Entering Prompt run with input:\n",
      "\u001b[0m{\n",
      "  \"history\": \"Human: My favourite color is purple, my favourite food is pizza.\\nAI: I understand. Your favourite color is purple, and your favourite food is pizza.\",\n",
      "  \"input\": \"What is my favourite color?\"\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[1:chain:RunnableSequence > 8:prompt:ChatPromptTemplate] [1ms] Exiting Prompt run with output:\n",
      "\u001b[0m{\n",
      "  \"lc\": 1,\n",
      "  \"type\": \"constructor\",\n",
      "  \"id\": [\n",
      "    \"langchain\",\n",
      "    \"prompts\",\n",
      "    \"chat\",\n",
      "    \"ChatPromptValue\"\n",
      "  ],\n",
      "  \"kwargs\": {\n",
      "    \"messages\": [\n",
      "      {\n",
      "        \"lc\": 1,\n",
      "        \"type\": \"constructor\",\n",
      "        \"id\": [\n",
      "          \"langchain\",\n",
      "          \"schema\",\n",
      "          \"messages\",\n",
      "          \"SystemMessage\"\n",
      "        ],\n",
      "        \"kwargs\": {\n",
      "          \"content\": \"Conversation Summary:\\nHuman: My favourite color is purple, my favourite food is pizza.\\nAI: I understand. Your favourite color is purple, and your favourite food is pizza.\",\n",
      "          \"additional_kwargs\": {}\n",
      "        }\n",
      "      },\n",
      "      {\n",
      "        \"lc\": 1,\n",
      "        \"type\": \"constructor\",\n",
      "        \"id\": [\n",
      "          \"langchain\",\n",
      "          \"schema\",\n",
      "          \"messages\",\n",
      "          \"HumanMessage\"\n",
      "        ],\n",
      "        \"kwargs\": {\n",
      "          \"content\": \"What is my favourite color?\",\n",
      "          \"additional_kwargs\": {}\n",
      "        }\n",
      "      }\n",
      "    ]\n",
      "  }\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[llm/start]\u001b[0m \u001b[1m[1:chain:RunnableSequence > 9:llm:ChatOpenAI] Entering LLM run with input:\n",
      "\u001b[0m{\n",
      "  \"prompts\": [\n",
      "    \"System: Conversation Summary:\\nHuman: My favourite color is purple, my favourite food is pizza.\\nAI: I understand. Your favourite color is purple, and your favourite food is pizza.\\nHuman: What is my favourite color?\"\n",
      "  ]\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[llm/end]\u001b[0m \u001b[1m[1:chain:RunnableSequence > 9:llm:ChatOpenAI] [931ms] Exiting LLM run with output:\n",
      "\u001b[0m{\n",
      "  \"generations\": [\n",
      "    [\n",
      "      {\n",
      "        \"text\": \"Your favourite color is purple.\",\n",
      "        \"generation_info\": {\n",
      "          \"finish_reason\": \"stop\",\n",
      "          \"logprobs\": null\n",
      "        },\n",
      "        \"type\": \"ChatGeneration\",\n",
      "        \"message\": {\n",
      "          \"lc\": 1,\n",
      "          \"type\": \"constructor\",\n",
      "          \"id\": [\n",
      "            \"langchain\",\n",
      "            \"schema\",\n",
      "            \"messages\",\n",
      "            \"AIMessage\"\n",
      "          ],\n",
      "          \"kwargs\": {\n",
      "            \"content\": \"Your favourite color is purple.\",\n",
      "            \"additional_kwargs\": {}\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "    ]\n",
      "  ],\n",
      "  \"llm_output\": {\n",
      "    \"token_usage\": {\n",
      "      \"completion_tokens\": 6,\n",
      "      \"prompt_tokens\": 52,\n",
      "      \"total_tokens\": 58\n",
      "    },\n",
      "    \"model_name\": \"gpt-3.5-turbo\",\n",
      "    \"system_fingerprint\": null\n",
      "  },\n",
      "  \"run\": null\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[1:chain:RunnableSequence] [1.91s] Exiting Chain run with output:\n",
      "\u001b[0m[outputs]\n",
      "content='Your favourite color is purple.'\n",
      "tokens: Tokens Used: 58\n",
      "\tPrompt Tokens: 52\n",
      "\tCompletion Tokens: 6\n",
      "Successful Requests: 1\n",
      "Total Cost (USD): $8.999999999999999e-05\n"
     ]
    }
   ],
   "source": [
    "with get_openai_callback() as cb:\n",
    "    result = chain.invoke(\"What is my favourite color?\")\n",
    "    print(f'{result}')\n",
    "    print(f'tokens: {cb}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = (\n",
    "    RunnablePassthrough.assign(\n",
    "        history=RunnableLambda(memory.load_memory_variables) | itemgetter(\"history\")\n",
    "    )\n",
    "    | prompt\n",
    "    | model\n",
    ")\n",
    "small_chain = (\n",
    "    RunnablePassthrough.assign(\n",
    "        history=RunnableLambda(memory.load_memory_variables) | itemgetter(\"history\")\n",
    "    )\n",
    "    | prompt\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Hello Bob! How can I assist you today?')"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs = {\"input\": \"hi im bob\"}\n",
    "response = chain.invoke(inputs)\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "memory.save_context(inputs, {\"output\": response.content})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'history': [HumanMessage(content='hi im bob'),\n",
       "  AIMessage(content='Hello Bob! How can I assist you today?')]}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "memory.load_memory_variables({})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tokens: Tokens Used: 47\n",
      "\tPrompt Tokens: 42\n",
      "\tCompletion Tokens: 5\n",
      "Successful Requests: 1\n",
      "Total Cost (USD): $7.3e-05\n"
     ]
    }
   ],
   "source": [
    "with get_openai_callback() as cb:\n",
    "    inputs = {\"input\": \"whats my name\"}\n",
    "    response = chain.invoke(inputs)\n",
    "    response\n",
    "    print(f'tokens: {cb}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatPromptValue(messages=[SystemMessage(content='You are a helpful chatbot'), HumanMessage(content='hi im bob'), AIMessage(content='Hello Bob! How can I assist you today?'), HumanMessage(content='whats my name')])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs = {\"input\": \"whats my name\"}\n",
    "small_chain.invoke(inputs)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
