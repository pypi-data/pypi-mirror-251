[
  {
    "title": "Get started | ü¶úÔ∏èüîó Langchain",
    "url": "https://python.langchain.com/docs/expression_language/get_started",
    "html": "Skip to main content\nü¶úÔ∏èüîó LangChain\nDocs\nUse cases\nIntegrations\nGuides\nAPI\nMore\nü¶úÔ∏èüîó\nChat\nSearch\nK\nGet started\nIntroduction\nInstallation\nQuickstart\nSecurity\nLangChain Expression Language\nGet started\nWhy use LCEL\nInterface\nHow to\nCookbook\nModules\nModel I/O\nRetrieval\nAgents\nChains\nMore\nLangServe\nLangSmith\nLangGraph\nLangChain Expression LanguageGet started\nGet started\n\nLCEL makes it easy to build complex chains from basic components, and supports out of the box functionality such as streaming, parallelism, and logging.\n\nBasic example: prompt + model + output parser‚Äã\n\nThe most basic and common use case is chaining a prompt template and a model together. To see how this works, let‚Äôs create a chain that takes a topic and generates a joke:\n\n%pip install ‚Äìupgrade ‚Äìquiet langchain-core langchain-community langchain-openai\n\nfrom langchain_core.output_parsers import StrOutputParser\nfrom langchain_core.prompts import ChatPromptTemplate\nfrom langchain_openai import ChatOpenAI\n\nprompt = ChatPromptTemplate.from_template(\"tell me a short joke about {topic}\")\nmodel = ChatOpenAI(model=\"gpt-4\")\noutput_parser = StrOutputParser()\n\nchain = prompt | model | output_parser\n\nchain.invoke({\"topic\": \"ice cream\"})\n\n\"Why don't ice creams ever get invited to parties?\\n\\nBecause they always drip when things heat up!\"\n\n\nNotice this line of this code, where we piece together then different components into a single chain using LCEL:\n\nchain = prompt | model | output_parser\n\n\nThe | symbol is similar to a unix pipe operator, which chains together the different components feeds the output from one component as input into the next component.\n\nIn this chain the user input is passed to the prompt template, then the prompt template output is passed to the model, then the model output is passed to the output parser. Let‚Äôs take a look at each component individually to really understand what‚Äôs going on.\n\n1. Prompt‚Äã\n\nprompt is a BasePromptTemplate, which means it takes in a dictionary of template variables and produces a PromptValue. A PromptValue is a wrapper around a completed prompt that can be passed to either an LLM (which takes a string as input) or ChatModel (which takes a sequence of messages as input). It can work with either language model type because it defines logic both for producing BaseMessages and for producing a string.\n\nprompt_value = prompt.invoke({\"topic\": \"ice cream\"})\nprompt_value\n\nChatPromptValue(messages=[HumanMessage(content='tell me a short joke about ice cream')])\n\nprompt_value.to_messages()\n\n[HumanMessage(content='tell me a short joke about ice cream')]\n\nprompt_value.to_string()\n\n'Human: tell me a short joke about ice cream'\n\n2. Model‚Äã\n\nThe PromptValue is then passed to model. In this case our model is a ChatModel, meaning it will output a BaseMessage.\n\nmessage = model.invoke(prompt_value)\nmessage\n\nAIMessage(content=\"Why don't ice creams ever get invited to parties?\\n\\nBecause they always bring a melt down!\")\n\n\nIf our model was an LLM, it would output a string.\n\nfrom langchain_openai.llms import OpenAI\n\nllm = OpenAI(model=\"gpt-3.5-turbo-instruct\")\nllm.invoke(prompt_value)\n\n'\\n\\nRobot: Why did the ice cream truck break down? Because it had a meltdown!'\n\n3. Output parser‚Äã\n\nAnd lastly we pass our model output to the output_parser, which is a BaseOutputParser meaning it takes either a string or a BaseMessage as input. The StrOutputParser specifically simple converts any input into a string.\n\noutput_parser.invoke(message)\n\n\"Why did the ice cream go to therapy? \\n\\nBecause it had too many toppings and couldn't find its cone-fidence!\"\n\n4. Entire Pipeline‚Äã\n\nTo follow the steps along:\n\nWe pass in user input on the desired topic as {\"topic\": \"ice cream\"}\nThe prompt component takes the user input, which is then used to construct a PromptValue after using the topic to construct the prompt.\nThe model component takes the generated prompt, and passes into the OpenAI LLM model for evaluation. The generated output from the model is a ChatMessage object.\nFinally, the output_parser component takes in a ChatMessage, and transforms this into a Python string, which is returned from the invoke method.\nDict\nPromptValue\nChatMessage\nString\nInput: topic=ice cream\nPromptTemplate\nChatModel\nStrOutputParser\nResult\n\nNote that if you‚Äôre curious about the output of any components, you can always test out a smaller version of the chain such as prompt or prompt | model to see the intermediate results:\n\ninput = {\"topic\": \"ice cream\"}\n\nprompt.invoke(input)\n# > ChatPromptValue(messages=[HumanMessage(content='tell me a short joke about ice cream')])\n\n(prompt | model).invoke(input)\n# > AIMessage(content=\"Why did the ice cream go to therapy?\\nBecause it had too many toppings and couldn't cone-trol itself!\")\n\nRAG Search Example‚Äã\n\nFor our next example, we want to run a retrieval-augmented generation chain to add some context when responding to questions.\n\n# Requires:\n# pip install langchain docarray tiktoken\n\nfrom langchain_community.vectorstores import DocArrayInMemorySearch\nfrom langchain_core.output_parsers import StrOutputParser\nfrom langchain_core.prompts import ChatPromptTemplate\nfrom langchain_core.runnables import RunnableParallel, RunnablePassthrough\nfrom langchain_openai.chat_models import ChatOpenAI\nfrom langchain_openai.embeddings import OpenAIEmbeddings\n\nvectorstore = DocArrayInMemorySearch.from_texts(\n    [\"harrison worked at kensho\", \"bears like to eat honey\"],\n    embedding=OpenAIEmbeddings(),\n)\nretriever = vectorstore.as_retriever()\n\ntemplate = \"\"\"Answer the question based only on the following context:\n{context}\n\nQuestion: {question}\n\"\"\"\nprompt = ChatPromptTemplate.from_template(template)\nmodel = ChatOpenAI()\noutput_parser = StrOutputParser()\n\nsetup_and_retrieval = RunnableParallel(\n    {\"context\": retriever, \"question\": RunnablePassthrough()}\n)\nchain = setup_and_retrieval | prompt | model | output_parser\n\nchain.invoke(\"where did harrison work?\")\n\n\nIn this case, the composed chain is:\n\nchain = setup_and_retrieval | prompt | model | output_parser\n\n\nTo explain this, we first can see that the prompt template above takes in context and question as values to be substituted in the prompt. Before building the prompt template, we want to retrieve relevant documents to the search and include them as part of the context.\n\nAs a preliminary step, we‚Äôve setup the retriever using an in memory store, which can retrieve documents based on a query. This is a runnable component as well that can be chained together with other components, but you can also try to run it separately:\n\nretriever.invoke(\"where did harrison work?\")\n\n\nWe then use the RunnableParallel to prepare the expected inputs into the prompt by using the entries for the retrieved documents as well as the original user question, using the retriever for document search, and RunnablePassthrough to pass the user‚Äôs question:\n\nsetup_and_retrieval = RunnableParallel(\n    {\"context\": retriever, \"question\": RunnablePassthrough()}\n)\n\n\nTo review, the complete chain is:\n\nsetup_and_retrieval = RunnableParallel(\n    {\"context\": retriever, \"question\": RunnablePassthrough()}\n)\nchain = setup_and_retrieval | prompt | model | output_parser\n\n\nWith the flow being:\n\nThe first steps create a RunnableParallel object with two entries. The first entry, context will include the document results fetched by the retriever. The second entry, question will contain the user‚Äôs original question. To pass on the question, we use RunnablePassthrough to copy this entry.\nFeed the dictionary from the step above to the prompt component. It then takes the user input which is question as well as the retrieved document which is context to construct a prompt and output a PromptValue.\nThe model component takes the generated prompt, and passes into the OpenAI LLM model for evaluation. The generated output from the model is a ChatMessage object.\nFinally, the output_parser component takes in a ChatMessage, and transforms this into a Python string, which is returned from the invoke method.\nQuestion\nQuestion\ncontext=retrieved docs\nquestion=Question\nPromptValue\nChatMessage\nString\nQuestion\nRunnableParallel\nRetriever\nRunnablePassThrough\nPromptTemplate\nChatModel\nStrOutputParser\nResult\nNext steps‚Äã\n\nWe recommend reading our Why use LCEL section next to see a side-by-side comparison of the code needed to produce common functionality with and without LCEL.\n\nPrevious\nLangChain Expression Language (LCEL)\nNext\nWhy use LCEL\nBasic example: prompt + model + output parser\n1. Prompt\n2. Model\n3. Output parser\n4. Entire Pipeline\nRAG Search Example\nNext steps\nCommunity\nDiscord\nTwitter\nGitHub\nPython\nJS/TS\nMore\nHomepage\nBlog\nCopyright ¬© 2024 LangChain, Inc."
  },
  {
    "title": "LangChain Expression Language (LCEL) | ü¶úÔ∏èüîó Langchain",
    "url": "https://python.langchain.com/docs/expression_language/",
    "html": "Skip to main content\nü¶úÔ∏èüîó LangChain\nDocs\nUse cases\nIntegrations\nGuides\nAPI\nMore\nü¶úÔ∏èüîó\nChat\nSearch\nK\nGet started\nIntroduction\nInstallation\nQuickstart\nSecurity\nLangChain Expression Language\nGet started\nWhy use LCEL\nInterface\nHow to\nCookbook\nModules\nModel I/O\nRetrieval\nAgents\nChains\nMore\nLangServe\nLangSmith\nLangGraph\nLangChain Expression Language\nLangChain Expression Language (LCEL)\n\nLangChain Expression Language, or LCEL, is a declarative way to easily compose chains together. LCEL was designed from day 1 to support putting prototypes in production, with no code changes, from the simplest ‚Äúprompt + LLM‚Äù chain to the most complex chains (we‚Äôve seen folks successfully run LCEL chains with 100s of steps in production). To highlight a few of the reasons you might want to use LCEL:\n\nStreaming support When you build your chains with LCEL you get the best possible time-to-first-token (time elapsed until the first chunk of output comes out). For some chains this means eg. we stream tokens straight from an LLM to a streaming output parser, and you get back parsed, incremental chunks of output at the same rate as the LLM provider outputs the raw tokens.\n\nAsync support Any chain built with LCEL can be called both with the synchronous API (eg. in your Jupyter notebook while prototyping) as well as with the asynchronous API (eg. in a LangServe server). This enables using the same code for prototypes and in production, with great performance, and the ability to handle many concurrent requests in the same server.\n\nOptimized parallel execution Whenever your LCEL chains have steps that can be executed in parallel (eg if you fetch documents from multiple retrievers) we automatically do it, both in the sync and the async interfaces, for the smallest possible latency.\n\nRetries and fallbacks Configure retries and fallbacks for any part of your LCEL chain. This is a great way to make your chains more reliable at scale. We‚Äôre currently working on adding streaming support for retries/fallbacks, so you can get the added reliability without any latency cost.\n\nAccess intermediate results For more complex chains it‚Äôs often very useful to access the results of intermediate steps even before the final output is produced. This can be used to let end-users know something is happening, or even just to debug your chain. You can stream intermediate results, and it‚Äôs available on every LangServe server.\n\nInput and output schemas Input and output schemas give every LCEL chain Pydantic and JSONSchema schemas inferred from the structure of your chain. This can be used for validation of inputs and outputs, and is an integral part of LangServe.\n\nSeamless LangSmith tracing integration As your chains get more and more complex, it becomes increasingly important to understand what exactly is happening at every step. With LCEL, all steps are automatically logged to LangSmith for maximum observability and debuggability.\n\nSeamless LangServe deployment integration Any chain created with LCEL can be easily deployed using LangServe.\n\nPrevious\nSecurity\nNext\nGet started\nCommunity\nDiscord\nTwitter\nGitHub\nPython\nJS/TS\nMore\nHomepage\nBlog\nCopyright ¬© 2024 LangChain, Inc."
  },
  {
    "title": "Security | ü¶úÔ∏èüîó Langchain",
    "url": "https://python.langchain.com/docs/security",
    "html": "Skip to main content\nü¶úÔ∏èüîó LangChain\nDocs\nUse cases\nIntegrations\nGuides\nAPI\nMore\nü¶úÔ∏èüîó\nChat\nSearch\nK\nGet started\nIntroduction\nInstallation\nQuickstart\nSecurity\nLangChain Expression Language\nGet started\nWhy use LCEL\nInterface\nHow to\nCookbook\nModules\nModel I/O\nRetrieval\nAgents\nChains\nMore\nLangServe\nLangSmith\nLangGraph\nGet startedSecurity\nSecurity\n\nLangChain has a large ecosystem of integrations with various external resources like local and remote file systems, APIs and databases. These integrations allow developers to create versatile applications that combine the power of LLMs with the ability to access, interact with and manipulate external resources.\n\nBest Practices‚Äã\n\nWhen building such applications developers should remember to follow good security practices:\n\nLimit Permissions: Scope permissions specifically to the application's need. Granting broad or excessive permissions can introduce significant security vulnerabilities. To avoid such vulnerabilities, consider using read-only credentials, disallowing access to sensitive resources, using sandboxing techniques (such as running inside a container), etc. as appropriate for your application.\nAnticipate Potential Misuse: Just as humans can err, so can Large Language Models (LLMs). Always assume that any system access or credentials may be used in any way allowed by the permissions they are assigned. For example, if a pair of database credentials allows deleting data, it‚Äôs safest to assume that any LLM able to use those credentials may in fact delete data.\nDefense in Depth: No security technique is perfect. Fine-tuning and good chain design can reduce, but not eliminate, the odds that a Large Language Model (LLM) may make a mistake. It‚Äôs best to combine multiple layered security approaches rather than relying on any single layer of defense to ensure security. For example: use both read-only permissions and sandboxing to ensure that LLMs are only able to access data that is explicitly meant for them to use.\n\nRisks of not doing so include, but are not limited to:\n\nData corruption or loss.\nUnauthorized access to confidential information.\nCompromised performance or availability of critical resources.\n\nExample scenarios with mitigation strategies:\n\nA user may ask an agent with access to the file system to delete files that should not be deleted or read the content of files that contain sensitive information. To mitigate, limit the agent to only use a specific directory and only allow it to read or write files that are safe to read or write. Consider further sandboxing the agent by running it in a container.\nA user may ask an agent with write access to an external API to write malicious data to the API, or delete data from that API. To mitigate, give the agent read-only API keys, or limit it to only use endpoints that are already resistant to such misuse.\nA user may ask an agent with access to a database to drop a table or mutate the schema. To mitigate, scope the credentials to only the tables that the agent needs to access and consider issuing READ-ONLY credentials.\n\nIf you're building applications that access external resources like file systems, APIs or databases, consider speaking with your company's security team to determine how to best design and secure your applications.\n\nReporting a Vulnerability‚Äã\n\nPlease report security vulnerabilities by email to security@langchain.dev. This will ensure the issue is promptly triaged and acted upon as needed.\n\nEnterprise solutions‚Äã\n\nLangChain may offer enterprise solutions for customers who have additional security requirements. Please contact us at sales@langchain.dev.\n\nPrevious\nQuickstart\nNext\nLangChain Expression Language (LCEL)\nBest Practices\nReporting a Vulnerability\nEnterprise solutions\nCommunity\nDiscord\nTwitter\nGitHub\nPython\nJS/TS\nMore\nHomepage\nBlog\nCopyright ¬© 2024 LangChain, Inc."
  },
  {
    "title": "Quickstart | ü¶úÔ∏èüîó Langchain",
    "url": "https://python.langchain.com/docs/get_started/quickstart",
    "html": "Skip to main content\nü¶úÔ∏èüîó LangChain\nDocs\nUse cases\nIntegrations\nGuides\nAPI\nMore\nü¶úÔ∏èüîó\nChat\nSearch\nK\nGet started\nIntroduction\nInstallation\nQuickstart\nSecurity\nLangChain Expression Language\nGet started\nWhy use LCEL\nInterface\nHow to\nCookbook\nModules\nModel I/O\nRetrieval\nAgents\nChains\nMore\nLangServe\nLangSmith\nLangGraph\nGet startedQuickstart\nQuickstart\n\nIn this quickstart we'll show you how to:\n\nGet setup with LangChain, LangSmith and LangServe\nUse the most basic and common components of LangChain: prompt templates, models, and output parsers\nUse LangChain Expression Language, the protocol that LangChain is built on and which facilitates component chaining\nBuild a simple application with LangChain\nTrace your application with LangSmith\nServe your application with LangServe\n\nThat's a fair amount to cover! Let's dive in.\n\nSetup‚Äã\nJupyter Notebook‚Äã\n\nThis guide (and most of the other guides in the documentation) use Jupyter notebooks and assume the reader is as well. Jupyter notebooks are perfect for learning how to work with LLM systems because often times things can go wrong (unexpected output, API down, etc) and going through guides in an interactive environment is a great way to better understand them.\n\nYou do not NEED to go through the guide in a Jupyter Notebook, but it is recommended. See here for instructions on how to install.\n\nInstallation‚Äã\n\nTo install LangChain run:\n\nPip\nConda\npip install langchain\n\n\nFor more details, see our Installation guide.\n\nLangSmith‚Äã\n\nMany of the applications you build with LangChain will contain multiple steps with multiple invocations of LLM calls. As these applications get more and more complex, it becomes crucial to be able to inspect what exactly is going on inside your chain or agent. The best way to do this is with LangSmith.\n\nNote that LangSmith is not needed, but it is helpful. If you do want to use LangSmith, after you sign up at the link above, make sure to set your environment variables to start logging traces:\n\nexport LANGCHAIN_TRACING_V2=\"true\"\nexport LANGCHAIN_API_KEY=\"...\"\n\nBuilding with LangChain‚Äã\n\nLangChain enables building application that connect external sources of data and computation to LLMs. In this quickstart, we will walk through a few different ways of doing that. We will start with a simple LLM chain, which just relies on information in the prompt template to respond. Next, we will build a retrieval chain, which fetches data from a separate database and passes that into the prompt template. We will then add in chat history, to create a conversation retrieval chain. This allows you interact in a chat manner with this LLM, so it remembers previous questions. Finally, we will build an agent - which utilizes and LLM to determine whether or not it needs to fetch data to answer questions. We will cover these at a high level, but there are lot of details to all of these! We will link to relevant docs.\n\nLLM Chain‚Äã\n\nFor this getting started guide, we will provide two options: using OpenAI (a popular model available via API) or using a local open source model.\n\nOpenAI\nLocal\n\nFirst we'll need to import the LangChain x OpenAI integration package.\n\npip install langchain-openai\n\n\nAccessing the API requires an API key, which you can get by creating an account and heading here. Once we have a key we'll want to set it as an environment variable by running:\n\nexport OPENAI_API_KEY=\"...\"\n\n\nWe can then initialize the model:\n\nfrom langchain_openai import ChatOpenAI\n\nllm = ChatOpenAI()\n\n\nIf you'd prefer not to set an environment variable you can pass the key in directly via the openai_api_key named parameter when initiating the OpenAI LLM class:\n\nfrom langchain_openai import ChatOpenAI\n\nllm = ChatOpenAI(openai_api_key=\"...\")\n\n\nOnce you've installed and initialized the LLM of your choice, we can try using it! Let's ask it what LangSmith is - this is something that wasn't present in the training data so it shouldn't have a very good response.\n\nllm.invoke(\"how can langsmith help with testing?\")\n\n\nWe can also guide it's response with a prompt template. Prompt templates are used to convert raw user input to a better input to the LLM.\n\nfrom langchain_core.prompts import ChatPromptTemplate\nprompt = ChatPromptTemplate.from_messages([\n    (\"system\", \"You are world class technical documentation writer.\"),\n    (\"user\", \"{input}\")\n])\n\n\nWe can now combine these into a simple LLM chain:\n\nchain = prompt | llm \n\n\nWe can now invoke it and ask the same question. It still won't know the answer, but it should respond in a more proper tone for a technical writer!\n\nchain.invoke({\"input\": \"how can langsmith help with testing?\"})\n\n\nThe output of a ChatModel (and therefore, of this chain) is a message. However, it's often much more convenient to work with strings. Let's add a simple output parser to convert the chat message to a string.\n\nfrom langchain_core.output_parsers import StrOutputParser\n\noutput_parser = StrOutputParser()\n\n\nWe can now add this to the previous chain:\n\nchain = prompt | llm | output_parser\n\n\nWe can now invoke it and ask the same question. The answer will now be a string (rather than a ChatMessage).\n\nchain.invoke({\"input\": \"how can langsmith help with testing?\"})\n\nDiving Deeper‚Äã\n\nWe've now successfully set up a basic LLM chain. We only touched on the basics of prompts, models, and output parsers - for a deeper dive into everything mentioned here, see this section of documentation.\n\nRetrieval Chain‚Äã\n\nIn order to properly answer the original question (\"how can langsmith help with testing?\"), we need to provide additional context to the LLM. We can do this via retrieval. Retrieval is useful when you have too much data to pass to the LLM directly. You can then use a retriever to fetch only the most relevant pieces and pass those in.\n\nIn this process, we will look up relevant documents from a Retriever and then pass them into the prompt. A Retriever can be backed by anything - a SQL table, the internet, etc - but in this instance we will populate a vector store and use that as a retriever. For more information on vectorstores, see this documentation.\n\nFirst, we need to load the data that we want to index. In order to do this, we will use the WebBaseLoader. This requires installing BeautifulSoup:\n\n```shell\npip install beautifulsoup4\n\n\nAfter that, we can import and use WebBaseLoader.\n\nfrom langchain_community.document_loaders import WebBaseLoader\nloader = WebBaseLoader(\"https://docs.smith.langchain.com/overview\")\n\ndocs = loader.load()\n\n\nNext, we need to index it into a vectorstore. This requires a few components, namely an embedding model and a vectorstore.\n\nFor embedding models, we once again provide examples for accessing via OpenAI or via local models.\n\nOpenAI\nLocal\nMake sure you have the `langchain_openai` package installed an the appropriate environment variables set (these are the same as needed for the LLM).\nfrom langchain_openai import OpenAIEmbeddings\n\nembeddings = OpenAIEmbeddings()\n\n\nNow, we can use this embedding model to ingest documents into a vectorstore. We will use a simple local vectorstore, FAISS, for simplicity's sake.\n\nFirst we need to install the required packages for that:\n\npip install faiss-cpu\n\n\nThen we can build our index:\n\nfrom langchain_community.vectorstores import FAISS\nfrom langchain.text_splitter import RecursiveCharacterTextSplitter\n\n\ntext_splitter = RecursiveCharacterTextSplitter()\ndocuments = text_splitter.split_documents(docs)\nvector = FAISS.from_documents(documents, embeddings)\n\n\nNow that we have this data indexed in a vectorstore, we will create a retrieval chain. This chain will take an incoming question, look up relevant documents, then pass those documents along with the original question into an LLM and ask it to answer the original question.\n\nFirst, let's set up the chain that takes a question and the retrieved documents and generates an answer.\n\nfrom langchain.chains.combine_documents import create_stuff_documents_chain\n\nprompt = ChatPromptTemplate.from_template(\"\"\"Answer the following question based only on the provided context:\n\n<context>\n{context}\n</context>\n\nQuestion: {input}\"\"\")\n\ndocument_chain = create_stuff_documents_chain(llm, prompt)\n\n\nIf we wanted to, we could run this ourselves by passing in documents directly:\n\nfrom langchain_core.documents import Document\n\ndocument_chain.invoke({\n    \"input\": \"how can langsmith help with testing?\",\n    \"context\": [Document(page_content=\"langsmith can let you visualize test results\")]\n})\n\n\nHowever, we want the documents to first come from the retriever we just set up. That way, for a given question we can use the retriever to dynamically select the most relevant documents and pass those in.\n\nfrom langchain.chains import create_retrieval_chain\n\nretriever = vector.as_retriever()\nretrieval_chain = create_retrieval_chain(retriever, document_chain)\n\n\nWe can now invoke this chain. This returns a dictionary - the response from the LLM is in the answer key\n\nresponse = retrieval_chain.invoke({\"input\": \"how can langsmith help with testing?\"})\nprint(response[\"answer\"])\n\n# LangSmith offers several features that can help with testing:...\n\n\nThis answer should be much more accurate!\n\nDiving Deeper‚Äã\n\nWe've now successfully set up a basic retrieval chain. We only touched on the basics of retrieval - for a deeper dive into everything mentioned here, see this section of documentation.\n\nConversation Retrieval Chain‚Äã\n\nThe chain we've created so far can only answer single questions. One of the main types of LLM applications that people are building are chat bots. So how do we turn this chain into one that can answer follow up questions?\n\nWe can still use the create_retrieval_chain function, but we need to change two things:\n\nThe retrieval method should now not just work on the most recent input, but rather should take the whole history into account.\nThe final LLM chain should likewise take the whole history into account\n\nUpdating Retrieval\n\nIn order to update retrieval, we will create a new chain. This chain will take in the most recent input (input) and the conversation history (chat_history) and use an LLM to generate a search query.\n\nfrom langchain.chains import create_history_aware_retriever\nfrom langchain_core.prompts import MessagesPlaceholder\n\n# First we need a prompt that we can pass into an LLM to generate this search query\n\nprompt = ChatPromptTemplate.from_messages([\n    MessagesPlaceholder(variable_name=\"chat_history\"),\n    (\"user\", \"{input}\"),\n    (\"user\", \"Given the above conversation, generate a search query to look up in order to get information relevant to the conversation\")\n])\nretriever_chain = create_history_aware_retriever(llm, retriever, prompt)\n\n\nWe can test this out by passing in an instance where the user is asking a follow up question.\n\nfrom langchain_core.messages import HumanMessage, AIMessage\n\nchat_history = [HumanMessage(content=\"Can LangSmith help test my LLM applications?\"), AIMessage(content=\"Yes!\")]\nretriever_chain.invoke({\n    \"chat_history\": chat_history,\n    \"input\": \"Tell me how\"\n})\n\n\nYou should see that this returns documents about testing in LangSmith. This is because the LLM generated a new query, combining the chat history with the follow up question.\n\nNow that we have this new retriever, we can create a new chain to continue the conversation with these retrieved documents in mind.\n\nprompt = ChatPromptTemplate.from_messages([\n    (\"system\", \"Answer the user's questions based on the below context:\\n\\n{context}\"),\n    MessagesPlaceholder(variable_name=\"chat_history\"),\n    (\"user\", \"{input}\"),\n])\ndocument_chain = create_stuff_documents_chain(llm, prompt)\n\nretrieval_chain = create_retrieval_chain(retriever_chain, document_chain)\n\n\nWe can now test this out end-to-end:\n\nchat_history = [HumanMessage(content=\"Can LangSmith help test my LLM applications?\"), AIMessage(content=\"Yes!\")]\nretrieval_chain.invoke({\n    \"chat_history\": chat_history,\n    \"input\": \"Tell me how\"\n})\n\n\nWe can see that this gives a coherent answer - we've successfully turned our retrieval chain into a chatbot!\n\nAgent‚Äã\n\nWe've so far create examples of chains - where each step is known ahead of time. The final thing we will create is an agent - where the LLM decides what steps to take.\n\nNOTE: for this example we will only show how to create an agent using OpenAI models, as local models are not reliable enough yet.\n\nOne of the first things to do when building an agent is to decide what tools it should have access to. For this example, we will give the agent access two tools:\n\nThe retriever we just created. This will let it easily answer questions about LangSmith\nA search tool. This will let it easily answer questions that require up to date information.\n\nFirst, let's set up a tool for the retriever we just created:\n\nfrom langchain.tools.retriever import create_retriever_tool\n\nretriever_tool = create_retriever_tool(\n    retriever,\n    \"langsmith_search\",\n    \"Search for information about LangSmith. For any questions about LangSmith, you must use this tool!\",\n)\n\n\nThe search tool that we will use is Tavily. This will require an API key (they have generous free tier). After creating it on their platform, you need to set it as an environment variable:\n\nexport TAVILY_API_KEY=...\n\n\nIf you do not want to set up an API key, you can skip creating this tool.\n\nfrom langchain_community.tools.tavily_search import TavilySearchResults\n\nsearch = TavilySearchResults()\n\n\nWe can now create a list of the tools we want to work with:\n\ntools = [retriever_tool, search]\n\n\nNow that we have the tools, we can create an agent to use them. We will go over this pretty quickly - for a deeper dive into what exactly is going on, check out the Agent's Getting Started documentation\n\nInstall langchain hub first\n\npip install langchainhub\n\n\nNow we can use it to get a predefined prompt\n\nfrom langchain_openai import ChatOpenAI\nfrom langchain import hub\nfrom langchain.agents import create_openai_functions_agent\nfrom langchain.agents import AgentExecutor\n\n# Get the prompt to use - you can modify this!\nprompt = hub.pull(\"hwchase17/openai-functions-agent\")\nllm = ChatOpenAI(model=\"gpt-3.5-turbo\", temperature=0)\nagent = create_openai_functions_agent(llm, tools, prompt)\nagent_executor = AgentExecutor(agent=agent, tools=tools, verbose=True)\n\n\nWe can now invoke the agent and see how it responds! We can ask it questions about LangSmith:\n\nagent_executor.invoke({\"input\": \"how can langsmith help with testing?\"})\n\n\nWe can ask it about the weather:\n\nagent_executor.invoke({\"input\": \"what is the weather in SF?\"})\n\n\nWe can have conversations with it:\n\nchat_history = [HumanMessage(content=\"Can LangSmith help test my LLM applications?\"), AIMessage(content=\"Yes!\")]\nagent_executor.invoke({\n    \"chat_history\": chat_history,\n    \"input\": \"Tell me how\"\n})\n\nDiving Deeper‚Äã\n\nWe've now successfully set up a basic agent. We only touched on the basics of agents - for a deeper dive into everything mentioned here, see this section of documentation.\n\nServing with LangServe‚Äã\n\nNow that we've built an application, we need to serve it. That's where LangServe comes in. LangServe helps developers deploy LangChain chains as a REST API. You do not need to use LangServe to use LangChain, but in this guide we'll show how you can deploy your app with LangServe.\n\nWhile the first part of this guide was intended to be run in a Jupyter Notebook, we will now move out of that. We will be creating a Python file and then interacting with it from the command line.\n\nInstall with:\n\npip install \"langserve[all]\"\n\nServer‚Äã\n\nTo create a server for our application we'll make a serve.py file. This will contain our logic for serving our application. It consists of three things:\n\nThe definition of our chain that we just built above\nOur FastAPI app\nA definition of a route from which to serve the chain, which is done with langserve.add_routes\n#!/usr/bin/env python\nfrom typing import List\n\nfrom fastapi import FastAPI\nfrom langchain_core.prompts import ChatPromptTemplate\nfrom langchain_openai import ChatOpenAI\nfrom langchain_community.document_loaders import WebBaseLoader\nfrom langchain_openai import OpenAIEmbeddings\nfrom langchain_community.vectorstores import FAISS\nfrom langchain.text_splitter import RecursiveCharacterTextSplitter\nfrom langchain.tools.retriever import create_retriever_tool\nfrom langchain_community.tools.tavily_search import TavilySearchResults\nfrom langchain_openai import ChatOpenAI\nfrom langchain import hub\nfrom langchain.agents import create_openai_functions_agent\nfrom langchain.agents import AgentExecutor\nfrom langchain.pydantic_v1 import BaseModel, Field\nfrom langchain_core.messages import BaseMessage\nfrom langserve import add_routes\n\n# 1. Load Retriever\nloader = WebBaseLoader(\"https://docs.smith.langchain.com/overview\")\ndocs = loader.load()\ntext_splitter = RecursiveCharacterTextSplitter()\ndocuments = text_splitter.split_documents(docs)\nembeddings = OpenAIEmbeddings()\nvector = FAISS.from_documents(documents, embeddings)\nretriever = vector.as_retriever()\n\n# 2. Create Tools\nretriever_tool = create_retriever_tool(\n    retriever,\n    \"langsmith_search\",\n    \"Search for information about LangSmith. For any questions about LangSmith, you must use this tool!\",\n)\nsearch = TavilySearchResults()\ntools = [retriever_tool, search]\n\n\n# 3. Create Agent\nprompt = hub.pull(\"hwchase17/openai-functions-agent\")\nllm = ChatOpenAI(model=\"gpt-3.5-turbo\", temperature=0)\nagent = create_openai_functions_agent(llm, tools, prompt)\nagent_executor = AgentExecutor(agent=agent, tools=tools, verbose=True)\n\n\n# 4. App definition\napp = FastAPI(\n  title=\"LangChain Server\",\n  version=\"1.0\",\n  description=\"A simple API server using LangChain's Runnable interfaces\",\n)\n\n# 5. Adding chain route\n\n# We need to add these input/output schemas because the current AgentExecutor\n# is lacking in schemas.\n\nclass Input(BaseModel):\n    input: str\n    chat_history: List[BaseMessage] = Field(\n        ...,\n        extra={\"widget\": {\"type\": \"chat\", \"input\": \"location\"}},\n    )\n\n\nclass Output(BaseModel):\n    output: str\n\nadd_routes(\n    app,\n    agent_executor.with_types(input_type=Input, output_type=Output),\n    path=\"/agent\",\n)\n\nif __name__ == \"__main__\":\n    import uvicorn\n\n    uvicorn.run(app, host=\"localhost\", port=8000)\n\n\nAnd that's it! If we execute this file:\n\npython serve.py\n\n\nwe should see our chain being served at localhost:8000.\n\nPlayground‚Äã\n\nEvery LangServe service comes with a simple built-in UI for configuring and invoking the application with streaming output and visibility into intermediate steps. Head to http://localhost:8000/agent/playground/ to try it out! Pass in the same question as before - \"how can langsmith help with testing?\" - and it should respond same as before.\n\nClient‚Äã\n\nNow let's set up a client for programmatically interacting with our service. We can easily do this with the [langserve.RemoteRunnable](/docs/langserve#client). Using this, we can interact with the served chain as if it were running client-side.\n\nfrom langserve import RemoteRunnable\n\nremote_chain = RemoteRunnable(\"http://localhost:8000/agent/\")\nremote_chain.invoke({\"input\": \"how can langsmith help with testing?\"})\n\n\nTo learn more about the many other features of LangServe head here.\n\nNext steps‚Äã\n\nWe've touched on how to build an application with LangChain, how to trace it with LangSmith, and how to serve it with LangServe. There are a lot more features in all three of these than we can cover here. To continue on your journey, we recommend you read the following (in order):\n\nAll of these features are backed by LangChain Expression Language (LCEL) - a way to chain these components together. Check out that documentation to better understand how to create custom chains.\nModel IO covers more details of prompts, LLMs, and output parsers.\nRetrieval covers more details of everything related to retrieval\nAgents covers details of everything related to agents\nExplore common end-to-end use cases and template applications\nRead up on LangSmith, the platform for debugging, testing, monitoring and more\nLearn more about serving your applications with LangServe\nPrevious\nInstallation\nNext\nSecurity\nSetup\nJupyter Notebook\nInstallation\nLangSmith\nBuilding with LangChain\nLLM Chain\nDiving Deeper\nRetrieval Chain\nDiving Deeper\nConversation Retrieval Chain\nAgent\nDiving Deeper\nServing with LangServe\nServer\nPlayground\nClient\nNext steps\nCommunity\nDiscord\nTwitter\nGitHub\nPython\nJS/TS\nMore\nHomepage\nBlog\nCopyright ¬© 2024 LangChain, Inc."
  },
  {
    "title": "Installation | ü¶úÔ∏èüîó Langchain",
    "url": "https://python.langchain.com/docs/get_started/installation",
    "html": "Skip to main content\nü¶úÔ∏èüîó LangChain\nDocs\nUse cases\nIntegrations\nGuides\nAPI\nMore\nü¶úÔ∏èüîó\nChat\nSearch\nK\nGet started\nIntroduction\nInstallation\nQuickstart\nSecurity\nLangChain Expression Language\nGet started\nWhy use LCEL\nInterface\nHow to\nCookbook\nModules\nModel I/O\nRetrieval\nAgents\nChains\nMore\nLangServe\nLangSmith\nLangGraph\nGet startedInstallation\nInstallation\nOfficial release‚Äã\n\nTo install LangChain run:\n\nPip\nConda\npip install langchain\n\n\nThis will install the bare minimum requirements of LangChain. A lot of the value of LangChain comes when integrating it with various model providers, datastores, etc. By default, the dependencies needed to do that are NOT installed. You will need to install the dependencies for specific integrations separately.\n\nFrom source‚Äã\n\nIf you want to install from source, you can do so by cloning the repo and be sure that the directory is PATH/TO/REPO/langchain/libs/langchain running:\n\npip install -e .\n\nLangChain community‚Äã\n\nThe langchain-community package contains third-party integrations. It is automatically installed by langchain, but can also be used separately. Install with:\n\npip install langchain-community\n\nLangChain core‚Äã\n\nThe langchain-core package contains base abstractions that the rest of the LangChain ecosystem uses, along with the LangChain Expression Language. It is automatically installed by langchain, but can also be used separately. Install with:\n\npip install langchain-core\n\nLangChain experimental‚Äã\n\nThe langchain-experimental package holds experimental LangChain code, intended for research and experimental uses. Install with:\n\npip install langchain-experimental\n\nLangServe‚Äã\n\nLangServe helps developers deploy LangChain runnables and chains as a REST API. LangServe is automatically installed by LangChain CLI. If not using LangChain CLI, install with:\n\npip install \"langserve[all]\"\n\n\nfor both client and server dependencies. Or pip install \"langserve[client]\" for client code, and pip install \"langserve[server]\" for server code.\n\nLangChain CLI‚Äã\n\nThe LangChain CLI is useful for working with LangChain templates and other LangServe projects. Install with:\n\npip install langchain-cli\n\nLangSmith SDK‚Äã\n\nThe LangSmith SDK is automatically installed by LangChain. If not using LangChain, install with:\n\npip install langsmith\n\nPrevious\nIntroduction\nNext\nQuickstart\nOfficial release\nFrom source\nLangChain community\nLangChain core\nLangChain experimental\nLangServe\nLangChain CLI\nLangSmith SDK\nCommunity\nDiscord\nTwitter\nGitHub\nPython\nJS/TS\nMore\nHomepage\nBlog\nCopyright ¬© 2024 LangChain, Inc."
  },
  {
    "title": "Get started | ü¶úÔ∏èüîó Langchain",
    "url": "https://python.langchain.com/docs/get_started",
    "html": "Skip to main content\nü¶úÔ∏èüîó LangChain\nDocs\nUse cases\nIntegrations\nGuides\nAPI\nMore\nü¶úÔ∏èüîó\nChat\nSearch\nK\nGet started\nIntroduction\nInstallation\nQuickstart\nSecurity\nLangChain Expression Language\nGet started\nWhy use LCEL\nInterface\nHow to\nCookbook\nModules\nModel I/O\nRetrieval\nAgents\nChains\nMore\nLangServe\nLangSmith\nLangGraph\nGet started\nGet started\n\nGet started with LangChain\n\nüìÑÔ∏è Introduction\n\nLangChain is a framework for developing applications powered by language models. It enables applications that:\n\nüìÑÔ∏è Installation\n\nOfficial release\n\nüìÑÔ∏è Quickstart\n\nIn this quickstart we'll show you how to:\n\nüìÑÔ∏è Security\n\nLangChain has a large ecosystem of integrations with various external resources like local and remote file systems, APIs and databases. These integrations allow developers to create versatile applications that combine the power of LLMs with the ability to access, interact with and manipulate external resources.\n\nNext\nIntroduction\nCommunity\nDiscord\nTwitter\nGitHub\nPython\nJS/TS\nMore\nHomepage\nBlog\nCopyright ¬© 2024 LangChain, Inc."
  },
  {
    "title": "YouTube videos | ü¶úÔ∏èüîó Langchain",
    "url": "https://python.langchain.com/docs/additional_resources/youtube",
    "html": "Skip to main content\nü¶úÔ∏èüîó LangChain\nDocs\nUse cases\nIntegrations\nGuides\nAPI\nMore\nü¶úÔ∏èüîó\nChat\nSearch\nK\nYouTube videos\n\n‚õì icon marks a new addition [last update 2023-09-21]\n\nOfficial LangChain YouTube channel‚Äã\nIntroduction to LangChain with Harrison Chase, creator of LangChain‚Äã\nBuilding the Future with LLMs, LangChain, & Pinecone by Pinecone\nLangChain and Weaviate with Harrison Chase and Bob van Luijt - Weaviate Podcast #36 by Weaviate ‚Ä¢ Vector Database\nLangChain Demo + Q&A with Harrison Chase by Full Stack Deep Learning\nLangChain Agents: Build Personal Assistants For Your Data (Q&A with Harrison Chase and Mayo Oshin) by Chat with data\nVideos (sorted by views)‚Äã\nUsing ChatGPT with YOUR OWN Data. This is magical. (LangChain OpenAI API) by TechLead\nFirst look - ChatGPT + WolframAlpha (GPT-3.5 and Wolfram|Alpha via LangChain by James Weaver) by Dr Alan D. Thompson\nLangChain explained - The hottest new Python framework by AssemblyAI\nChatbot with INFINITE MEMORY using OpenAI & Pinecone - GPT-3, Embeddings, ADA, Vector DB, Semantic by David Shapiro ~ AI\nLangChain for LLMs is... basically just an Ansible playbook by David Shapiro ~ AI\nBuild your own LLM Apps with LangChain & GPT-Index by 1littlecoder\nBabyAGI - New System of Autonomous AI Agents with LangChain by 1littlecoder\nRun BabyAGI with Langchain Agents (with Python Code) by 1littlecoder\nHow to Use Langchain With Zapier | Write and Send Email with GPT-3 | OpenAI API Tutorial by StarMorph AI\nUse Your Locally Stored Files To Get Response From GPT - OpenAI | Langchain | Python by Shweta Lodha\nLangchain JS | How to Use GPT-3, GPT-4 to Reference your own Data | OpenAI Embeddings Intro by StarMorph AI\nThe easiest way to work with large language models | Learn LangChain in 10min by Sophia Yang\n4 Autonomous AI Agents: ‚ÄúWestworld‚Äù simulation BabyAGI, AutoGPT, Camel, LangChain by Sophia Yang\nAI CAN SEARCH THE INTERNET? Langchain Agents + OpenAI ChatGPT by tylerwhatsgood\nQuery Your Data with GPT-4 | Embeddings, Vector Databases | Langchain JS Knowledgebase by StarMorph AI\nWeaviate + LangChain for LLM apps presented by Erika Cardenas by Weaviate ‚Ä¢ Vector Database\nLangchain Overview ‚Äî How to Use Langchain & ChatGPT by Python In Office\nLangchain Overview - How to Use Langchain & ChatGPT by Python In Office\nLangChain Tutorials by Edrick:\nLangChain, Chroma DB, OpenAI Beginner Guide | ChatGPT with your PDF\nLangChain 101: The Complete Beginner's Guide\nCustom langchain Agent & Tools with memory. Turn any Python function into langchain tool with Gpt 3 by echohive\nBuilding AI LLM Apps with LangChain (and more?) - LIVE STREAM by Nicholas Renotte\nChatGPT with any YouTube video using langchain and chromadb by echohive\nHow to Talk to a PDF using LangChain and ChatGPT by Automata Learning Lab\nLangchain Document Loaders Part 1: Unstructured Files by Merk\nLangChain - Prompt Templates (what all the best prompt engineers use) by Nick Daigler\nLangChain. Crear aplicaciones Python impulsadas por GPT by Jes√∫s Conde\nEasiest Way to Use GPT In Your Products | LangChain Basics Tutorial by Rachel Woods\nBabyAGI + GPT-4 Langchain Agent with Internet Access by tylerwhatsgood\nLearning LLM Agents. How does it actually work? LangChain, AutoGPT & OpenAI by Arnoldas Kemeklis\nGet Started with LangChain in Node.js by Developers Digest\nLangChain + OpenAI tutorial: Building a Q&A system w/ own text data by Samuel Chan\nLangchain + Zapier Agent by Merk\nConnecting the Internet with ChatGPT (LLMs) using Langchain And Answers Your Questions by Kamalraj M M\nBuild More Powerful LLM Applications for Business‚Äôs with LangChain (Beginners Guide) by No Code Blackbox\nLangFlow LLM Agent Demo for ü¶úüîóLangChain by Cobus Greyling\nChatbot Factory: Streamline Python Chatbot Creation with LLMs and Langchain by Finxter\nLangChain Tutorial - ChatGPT mit eigenen Daten by Coding Crashkurse\nChat with a CSV | LangChain Agents Tutorial (Beginners) by GoDataProf\nIntrodu√ß√£o ao Langchain - #Cortes - Live DataHackers by Prof. Jo√£o Gabriel Lima\nLangChain: Level up ChatGPT !? | LangChain Tutorial Part 1 by Code Affinity\nKI schreibt krasses Youtube Skript üò≤üò≥ | LangChain Tutorial Deutsch by SimpleKI\nChat with Audio: Langchain, Chroma DB, OpenAI, and Assembly AI by AI Anytime\nQA over documents with Auto vector index selection with Langchain router chains by echohive\nBuild your own custom LLM application with Bubble.io & Langchain (No Code & Beginner friendly) by No Code Blackbox\nSimple App to Question Your Docs: Leveraging Streamlit, Hugging Face Spaces, LangChain, and Claude! by Chris Alexiuk\nLANGCHAIN AI- ConstitutionalChainAI + Databutton AI ASSISTANT Web App by Avra\nLANGCHAIN AI AUTONOMOUS AGENT WEB APP - üë∂ BABY AGI ü§ñ with EMAIL AUTOMATION using DATABUTTON by Avra\nThe Future of Data Analysis: Using A.I. Models in Data Analysis (LangChain) by Absent Data\nMemory in LangChain | Deep dive (python) by Eden Marco\n9 LangChain UseCases | Beginner's Guide | 2023 by Data Science Basics\nUse Large Language Models in Jupyter Notebook | LangChain | Agents & Indexes by Abhinaw Tiwari\nHow to Talk to Your Langchain Agent | 11 Labs + Whisper by VRSEN\nLangChain Deep Dive: 5 FUN AI App Ideas To Build Quickly and Easily by James NoCode\nLangChain 101: Models by Mckay Wrigley\nLangChain with JavaScript Tutorial #1 | Setup & Using LLMs by Leon van Zyl\nLangChain Overview & Tutorial for Beginners: Build Powerful AI Apps Quickly & Easily (ZERO CODE) by James NoCode\nLangChain In Action: Real-World Use Case With Step-by-Step Tutorial by Rabbitmetrics\nSummarizing and Querying Multiple Papers with LangChain by Automata Learning Lab\nUsing Langchain (and Replit) through Tana, ask Google/Wikipedia/Wolfram Alpha to fill out a table by Stian H√•klev\nLangchain PDF App (GUI) | Create a ChatGPT For Your PDF in Python by Alejandro AO - Software & Ai\nAuto-GPT with LangChain üî• | Create Your Own Personal AI Assistant by Data Science Basics\nCreate Your OWN Slack AI Assistant with Python & LangChain by Dave Ebbelaar\nHow to Create LOCAL Chatbots with GPT4All and LangChain [Full Guide] by Liam Ottley\nBuild a Multilingual PDF Search App with LangChain, Cohere and Bubble by Menlo Park Lab\nBuilding a LangChain Agent (code-free!) Using Bubble and Flowise by Menlo Park Lab\nBuild a LangChain-based Semantic PDF Search App with No-Code Tools Bubble and Flowise by Menlo Park Lab\nLangChain Memory Tutorial | Building a ChatGPT Clone in Python by Alejandro AO - Software & Ai\nChatGPT For Your DATA | Chat with Multiple Documents Using LangChain by Data Science Basics\nLlama Index: Chat with Documentation using URL Loader by Merk\nUsing OpenAI, LangChain, and Gradio to Build Custom GenAI Applications by David Hundley\nLangChain, Chroma DB, OpenAI Beginner Guide | ChatGPT with your PDF\nBuild AI chatbot with custom knowledge base using OpenAI API and GPT Index by Irina Nik\nBuild Your Own Auto-GPT Apps with LangChain (Python Tutorial) by Dave Ebbelaar\nChat with Multiple PDFs | LangChain App Tutorial in Python (Free LLMs and Embeddings) by Alejandro AO - Software & Ai\nChat with a CSV | LangChain Agents Tutorial (Beginners) by Alejandro AO - Software & Ai\nCreate Your Own ChatGPT with PDF Data in 5 Minutes (LangChain Tutorial) by Liam Ottley\nBuild a Custom Chatbot with OpenAI: GPT-Index & LangChain | Step-by-Step Tutorial by Fabrikod\nFlowise is an open-source no-code UI visual tool to build ü¶úüîóLangChain applications by Cobus Greyling\nLangChain & GPT 4 For Data Analysis: The Pandas Dataframe Agent by Rabbitmetrics\nGirlfriendGPT - AI girlfriend with LangChain by Toolfinder AI\nHow to build with Langchain 10x easier | ‚õìÔ∏è LangFlow & Flowise by AI Jason\nGetting Started With LangChain In 20 Minutes- Build Celebrity Search Application by Krish Naik\n‚õì Vector Embeddings Tutorial ‚Äì Code Your Own AI Assistant with GPT-4 API + LangChain + NLP by FreeCodeCamp.org\n‚õì Fully LOCAL Llama 2 Q&A with LangChain by 1littlecoder\n‚õì Fully LOCAL Llama 2 Langchain on CPU by 1littlecoder\n‚õì Build LangChain Audio Apps with Python in 5 Minutes by AssemblyAI\n‚õì Voiceflow & Flowise: Want to Beat Competition? New Tutorial with Real AI Chatbot by AI SIMP\n‚õì THIS Is How You Build Production-Ready AI Apps (LangSmith Tutorial) by Dave Ebbelaar\n‚õì Build POWERFUL LLM Bots EASILY with Your Own Data - Embedchain - Langchain 2.0? (Tutorial) by WorldofAI\n‚õì Code Llama powered Gradio App for Coding: Runs on CPU by AI Anytime\n‚õì LangChain Complete Course in One Video | Develop LangChain (AI) Based Solutions for Your Business by UBprogrammer\n‚õì How to Run LLaMA Locally on CPU or GPU | Python & Langchain & CTransformers Guide by Code With Prince\n‚õì PyData Heidelberg #11 - TimeSeries Forecasting & LLM Langchain by PyData\n‚õì Prompt Engineering in Web Development | Using LangChain and Templates with OpenAI by Akamai Developer\n‚õì Retrieval-Augmented Generation (RAG) using LangChain and Pinecone - The RAG Special Episode by Generative AI and Data Science On AWS\n‚õì LLAMA2 70b-chat Multiple Documents Chatbot with Langchain & Streamlit |All OPEN SOURCE|Replicate API by DataInsightEdge\n‚õì Chatting with 44K Fashion Products: LangChain Opportunities and Pitfalls by Rabbitmetrics\n‚õì Structured Data Extraction from ChatGPT with LangChain by MG\n‚õì Chat with Multiple PDFs using Llama 2, Pinecone and LangChain (Free LLMs and Embeddings) by Muhammad Moin\n‚õì Integrate Audio into LangChain.js apps in 5 Minutes by AssemblyAI\n‚õì ChatGPT for your data with Local LLM by Jacob Jedryszek\n‚õì Training Chatgpt with your personal data using langchain step by step in detail by NextGen Machines\n‚õì Use ANY language in LangSmith with REST by Nerding I/O\n‚õì How to Leverage the Full Potential of LLMs for Your Business with Langchain - Leon Ruddat by PyData\n‚õì ChatCSV App: Chat with CSV files using LangChain and Llama 2 by Muhammad Moin\nPrompt Engineering and LangChain by Venelin Valkov‚Äã\nGetting Started with LangChain: Load Custom Data, Run OpenAI Models, Embeddings and ChatGPT\nLoaders, Indexes & Vectorstores in LangChain: Question Answering on PDF files with ChatGPT\nLangChain Models: ChatGPT, Flan Alpaca, OpenAI Embeddings, Prompt Templates & Streaming\nLangChain Chains: Use ChatGPT to Build Conversational Agents, Summaries and Q&A on Text With LLMs\nAnalyze Custom CSV Data with GPT-4 using Langchain\nBuild ChatGPT Chatbots with LangChain Memory: Understanding and Implementing Memory in Conversations\n\n‚õì icon marks a new addition [last update 2023-09-21]\n\nOfficial LangChain YouTube channel\nIntroduction to LangChain with Harrison Chase, creator of LangChain\nVideos (sorted by views)\nPrompt Engineering and LangChain by Venelin Valkov\nCommunity\nDiscord\nTwitter\nGitHub\nPython\nJS/TS\nMore\nHomepage\nBlog\nCopyright ¬© 2024 LangChain, Inc."
  },
  {
    "title": "Tutorials | ü¶úÔ∏èüîó Langchain",
    "url": "https://python.langchain.com/docs/additional_resources/tutorials",
    "html": "Skip to main content\nü¶úÔ∏èüîó LangChain\nDocs\nUse cases\nIntegrations\nGuides\nAPI\nMore\nü¶úÔ∏èüîó\nChat\nSearch\nK\nTutorials\n\nBelow are links to tutorials and courses on LangChain. For written guides on common use cases for LangChain, check out the use cases guides.\n\n‚õì icon marks a new addition [last update 2023-09-21]\n\nLangChain on Wikipedia‚Äã\nBooks‚Äã\n‚õìGenerative AI with LangChain by Ben Auffrath, ¬©Ô∏è 2023 Packt Publishing‚Äã\nDeepLearning.AI courses‚Äã\n\nby Harrison Chase and Andrew Ng\n\nLangChain for LLM Application Development\nLangChain Chat with Your Data\n‚õì Functions, Tools and Agents with LangChain\nHandbook‚Äã\n\nLangChain AI Handbook By James Briggs and Francisco Ingham\n\nShort Tutorials‚Äã\n\nLangChain Explained in 13 Minutes | QuickStart Tutorial for Beginners by Rabbitmetrics\n\nLangChain Crash Course: Build an AutoGPT app in 25 minutes by Nicholas Renotte\n\nLangChain Crash Course - Build apps with language models by Patrick Loeber\n\nTutorials‚Äã\nLangChain for Gen AI and LLMs by James Briggs‚Äã\n#1 Getting Started with GPT-3 vs. Open Source LLMs\n#2 Prompt Templates for GPT 3.5 and other LLMs\n#3 LLM Chains using GPT 3.5 and other LLMs\nLangChain Data Loaders, Tokenizers, Chunking, and Datasets - Data Prep 101\n#4 Chatbot Memory for Chat-GPT, Davinci + other LLMs\n#5 Chat with OpenAI in LangChain\n#6 Fixing LLM Hallucinations with Retrieval Augmentation in LangChain\n#7 LangChain Agents Deep Dive with GPT 3.5\n#8 Create Custom Tools for Chatbots in LangChain\n#9 Build Conversational Agents with Vector DBs\nUsing NEW MPT-7B in Hugging Face and LangChain\nMPT-30B Chatbot with LangChain\n‚õì Fine-tuning OpenAI's GPT 3.5 for LangChain Agents\n‚õì Chatbots with RAG: LangChain Full Walkthrough\nLangChain 101 by Greg Kamradt (Data Indy)‚Äã\nWhat Is LangChain? - LangChain + ChatGPT Overview\nQuickstart Guide\nBeginner's Guide To 7 Essential Concepts\nBeginner's Guide To 9 Use Cases\nAgents Overview + Google Searches\nOpenAI + Wolfram Alpha\nAsk Questions On Your Custom (or Private) Files\nConnect Google Drive Files To OpenAI\nYouTube Transcripts + OpenAI\nQuestion A 300 Page Book (w/ OpenAI + Pinecone)\nWorkaround OpenAI's Token Limit With Chain Types\nBuild Your Own OpenAI + LangChain Web App in 23 Minutes\nWorking With The New ChatGPT API\nOpenAI + LangChain Wrote Me 100 Custom Sales Emails\nStructured Output From OpenAI (Clean Dirty Data)\nConnect OpenAI To +5,000 Tools (LangChain + Zapier)\nUse LLMs To Extract Data From Text (Expert Mode)\nExtract Insights From Interview Transcripts Using LLMs\n5 Levels Of LLM Summarizing: Novice to Expert\nControl Tone & Writing Style Of Your LLM Output\nBuild Your Own AI Twitter Bot Using LLMs\nChatGPT made my interview questions for me (Streamlit + LangChain)\nFunction Calling via ChatGPT API - First Look With LangChain\nExtract Topics From Video/Audio With LLMs (Topic Modeling w/ LangChain)\nLangChain How to and guides by Sam Witteveen‚Äã\nLangChain Basics - LLMs & PromptTemplates with Colab\nLangChain Basics - Tools and Chains\nChatGPT API Announcement & Code Walkthrough with LangChain\nConversations with Memory (explanation & code walkthrough)\nChat with Flan20B\nUsing Hugging Face Models locally (code walkthrough)\nPAL: Program-aided Language Models with LangChain code\nBuilding a Summarization System with LangChain and GPT-3 - Part 1\nBuilding a Summarization System with LangChain and GPT-3 - Part 2\nMicrosoft's Visual ChatGPT using LangChain\nLangChain Agents - Joining Tools and Chains with Decisions\nComparing LLMs with LangChain\nUsing Constitutional AI in LangChain\nTalking to Alpaca with LangChain - Creating an Alpaca Chatbot\nTalk to your CSV & Excel with LangChain\nBabyAGI: Discover the Power of Task-Driven Autonomous Agents!\nImprove your BabyAGI with LangChain\nMaster PDF Chat with LangChain - Your essential guide to queries on documents\nUsing LangChain with DuckDuckGO, Wikipedia & PythonREPL Tools\nBuilding Custom Tools and Agents with LangChain (gpt-3.5-turbo)\nLangChain Retrieval QA Over Multiple Files with ChromaDB\nLangChain Retrieval QA with Instructor Embeddings & ChromaDB for PDFs\nLangChain + Retrieval Local LLMs for Retrieval QA - No OpenAI!!!\nCamel + LangChain for Synthetic Data & Market Research\nInformation Extraction with LangChain & Kor\nConverting a LangChain App from OpenAI to OpenSource\nUsing LangChain Output Parsers to get what you want out of LLMs\nBuilding a LangChain Custom Medical Agent with Memory\nUnderstanding ReACT with LangChain\nOpenAI Functions + LangChain : Building a Multi Tool Agent\nWhat can you do with 16K tokens in LangChain?\nTagging and Extraction - Classification using OpenAI Functions\nHOW to Make Conversational Form with LangChain\n‚õì Claude-2 meets LangChain!\n‚õì PaLM 2 Meets LangChain\n‚õì LLaMA2 with LangChain - Basics | LangChain TUTORIAL\n‚õì Serving LLaMA2 with Replicate\n‚õì NEW LangChain Expression Language\n‚õì Building a RCI Chain for Agents with LangChain Expression Language\n‚õì How to Run LLaMA-2-70B on the Together AI\n‚õì RetrievalQA with LLaMA 2 70b & Chroma DB\n‚õì How to use BGE Embeddings for LangChain\n‚õì How to use Custom Prompts for RetrievalQA on LLaMA-2 7B\nLangChain by Prompt Engineering‚Äã\nLangChain Crash Course ‚Äî All You Need to Know to Build Powerful Apps with LLMs\nWorking with MULTIPLE PDF Files in LangChain: ChatGPT for your Data\nChatGPT for YOUR OWN PDF files with LangChain\nTalk to YOUR DATA without OpenAI APIs: LangChain\nLangChain: PDF Chat App (GUI) | ChatGPT for Your PDF FILES\nLangFlow: Build Chatbots without Writing Code\nLangChain: Giving Memory to LLMs\nBEST OPEN Alternative to OPENAI's EMBEDDINGs for Retrieval QA: LangChain\nLangChain: Run Language Models Locally - Hugging Face Models\n‚õì Slash API Costs: Mastering Caching for LLM Applications\n‚õì Avoid PROMPT INJECTION with Constitutional AI - LangChain\nLangChain by Chat with data‚Äã\nLangChain Beginner's Tutorial for Typescript/Javascript\nGPT-4 Tutorial: How to Chat With Multiple PDF Files (~1000 pages of Tesla's 10-K Annual Reports)\nGPT-4 & LangChain Tutorial: How to Chat With A 56-Page PDF Document (w/Pinecone)\nLangChain & Supabase Tutorial: How to Build a ChatGPT Chatbot For Your Website\nLangChain Agents: Build Personal Assistants For Your Data (Q&A with Harrison Chase and Mayo Oshin)\nCodebase Analysis‚Äã\nCodebase Analysis: Langchain Agents\n\n‚õì icon marks a new addition [last update 2023-09-21]\n\nLangChain on Wikipedia\nBooks\nDeepLearning.AI courses\nHandbook\nShort Tutorials\nTutorials\nLangChain for Gen AI and LLMs by James Briggs\nLangChain 101 by Greg Kamradt (Data Indy)\nLangChain How to and guides by Sam Witteveen\nLangChain by Prompt Engineering\nLangChain by Chat with data\nCodebase Analysis\nCommunity\nDiscord\nTwitter\nGitHub\nPython\nJS/TS\nMore\nHomepage\nBlog\nCopyright ¬© 2024 LangChain, Inc."
  },
  {
    "title": "Templates | ü¶úÔ∏èüîó Langchain",
    "url": "https://python.langchain.com/docs/templates/",
    "html": "Skip to main content\nü¶úÔ∏èüîó LangChain\nDocs\nUse cases\nIntegrations\nGuides\nAPI\nMore\nü¶úÔ∏èüîó\nChat\nSearch\nK\nTemplates\nanthropic-iterative-search\nbasic-critique-revise\nBedrock JCVD üï∫ü•ã\ncassandra-entomology-rag\ncassandra-synonym-caching\nChain-of-Note (Wikipedia)\nChat Bot Feedback Template\ncohere-librarian\ncsv-agent\nelastic-query-generator\nextraction-anthropic-functions\nextraction-openai-functions\nguardrails-output-parser\nHybrid Search in Weaviate\nhyde\nllama2-functions\nmongo-parent-document-retrieval\nneo4j-advanced-rag\nneo4j-cypher-ft\nneo4j-cypher-memory\nneo4j_cypher\nneo4j-generation\nneo4j-parent\nneo4j-semantic-layer\nneo4j-vector-memory\nnvidia-rag-canonical\nOpenAI Functions Agent - Gmail\nopenai-functions-agent\nopenai-functions-tool-retrieval-agent\npii-protected-chatbot\npirate-speak-configurable\npirate-speak\nplate-chain\npropositional-retrieval\npython-lint\nrag-astradb\nrag-aws-bedrock\nrag-aws-kendra\nrag-chroma-multi-modal-multi-vector\nrag-chroma-multi-modal\nrag-chroma-private\nrag-chroma\nrag-codellama-fireworks\nrag-conversation-zep\nrag-conversation\nrag-elasticsearch\nrag-fusion\nrag-gemini-multi-modal\nrag-google-cloud-sensitive-data-protection\nrag-google-cloud-vertexai-search\nrag-gpt-crawler\nrag-matching-engine\nrag-momento-vector-index\nrag-mongo\nRAG with Multiple Indexes (Fusion)\nRAG with Multiple Indexes (Routing)\nrag-multi-modal-local\nrag-multi-modal-mv-local\nrag-ollama-multi-query\nrag-opensearch\nrag-pinecone-multi-query\nrag-pinecone-rerank\nrag-pinecone\nrag-redis\nrag-self-query\nrag-semi-structured\nrag-singlestoredb\nrag_supabase\nrag-timescale-conversation\nRAG with Timescale Vector using hybrid search\nrag-vectara-multiquery\nrag-vectara\nrag-weaviate\nresearch-assistant\nretrieval-agent\nrewrite_retrieve_read\nLangchain - Robocorp Action Server\nself-query-qdrant\nself-query-supabase\nskeleton-of-thought\nsolo-performance-prompting-agent\nsql-llama2\nsql-llamacpp\nsql-ollama\nsql-pgvector\nsql-research-assistant\nstepback-qa-prompting\nsummarize-anthropic\nvertexai-chuck-norris\nxml-agent\nTemplates\nTemplates\n\nHighlighting a few different categories of templates\n\n‚≠ê Popular‚Äã\n\nThese are some of the more popular templates to get started with.\n\nRetrieval Augmented Generation Chatbot: Build a chatbot over your data. Defaults to OpenAI and Pinecone.\nExtraction with OpenAI Functions: Do extraction of structured data from unstructured data. Uses OpenAI function calling.\nLocal Retrieval Augmented Generation: Build a chatbot over your data. Uses only local tooling: Ollama, GPT4all, Chroma.\nOpenAI Functions Agent: Build a chatbot that can take actions. Uses OpenAI function calling and Tavily.\nXML Agent: Build a chatbot that can take actions. Uses Anthropic and You.com.\nüì• Advanced Retrieval‚Äã\n\nThese templates cover advanced retrieval techniques, which can be used for chat and QA over databases or documents.\n\nReranking: This retrieval technique uses Cohere's reranking endpoint to rerank documents from an initial retrieval step.\nAnthropic Iterative Search: This retrieval technique uses iterative prompting to determine what to retrieve and whether the retriever documents are good enough.\nParent Document Retrieval using Neo4j or MongoDB: This retrieval technique stores embeddings for smaller chunks, but then returns larger chunks to pass to the model for generation.\nSemi-Structured RAG: The template shows how to do retrieval over semi-structured data (e.g. data that involves both text and tables).\nTemporal RAG: The template shows how to do hybrid search over data with a time-based component using Timescale Vector.\nüîçAdvanced Retrieval - Query Transformation‚Äã\n\nA selection of advanced retrieval methods that involve transforming the original user query, which can improve retrieval quality.\n\nHypothetical Document Embeddings: A retrieval technique that generates a hypothetical document for a given query, and then uses the embedding of that document to do semantic search. Paper.\nRewrite-Retrieve-Read: A retrieval technique that rewrites a given query before passing it to a search engine. Paper.\nStep-back QA Prompting: A retrieval technique that generates a \"step-back\" question and then retrieves documents relevant to both that question and the original question. Paper.\nRAG-Fusion: A retrieval technique that generates multiple queries and then reranks the retrieved documents using reciprocal rank fusion. Article.\nMulti-Query Retriever: This retrieval technique uses an LLM to generate multiple queries and then fetches documents for all queries.\nüß†Advanced Retrieval - Query Construction‚Äã\n\nA selection of advanced retrieval methods that involve constructing a query in a separate DSL from natural language, which enable natural language chat over various structured databases.\n\nElastic Query Generator: Generate elastic search queries from natural language.\nNeo4j Cypher Generation: Generate cypher statements from natural language. Available with a \"full text\" option as well.\nSupabase Self Query: Parse a natural language query into a semantic query as well as a metadata filter for Supabase.\nü¶ô OSS Models‚Äã\n\nThese templates use OSS models, which enable privacy for sensitive data.\n\nLocal Retrieval Augmented Generation: Build a chatbot over your data. Uses only local tooling: Ollama, GPT4all, Chroma.\nSQL Question Answering (Replicate): Question answering over a SQL database, using Llama2 hosted on Replicate.\nSQL Question Answering (LlamaCpp): Question answering over a SQL database, using Llama2 through LlamaCpp.\nSQL Question Answering (Ollama): Question answering over a SQL database, using Llama2 through Ollama.\n‚õèÔ∏è Extraction‚Äã\n\nThese templates extract data in a structured format based upon a user-specified schema.\n\nExtraction Using OpenAI Functions: Extract information from text using OpenAI Function Calling.\nExtraction Using Anthropic Functions: Extract information from text using a LangChain wrapper around the Anthropic endpoints intended to simulate function calling.\nExtract BioTech Plate Data: Extract microplate data from messy Excel spreadsheets into a more normalized format.\n‚õèÔ∏èSummarization and tagging‚Äã\n\nThese templates summarize or categorize documents and text.\n\nSummarization using Anthropic: Uses Anthropic's Claude2 to summarize long documents.\nü§ñ Agents‚Äã\n\nThese templates build chatbots that can take actions, helping to automate tasks.\n\nOpenAI Functions Agent: Build a chatbot that can take actions. Uses OpenAI function calling and Tavily.\nXML Agent: Build a chatbot that can take actions. Uses Anthropic and You.com.\nüö® Safety and evaluation‚Äã\n\nThese templates enable moderation or evaluation of LLM outputs.\n\nGuardrails Output Parser: Use guardrails-ai to validate LLM output.\nChatbot Feedback: Use LangSmith to evaluate chatbot responses.\nNext\nanthropic-iterative-search\n‚≠ê Popular\nüì• Advanced Retrieval\nüîçAdvanced Retrieval - Query Transformation\nüß†Advanced Retrieval - Query Construction\nü¶ô OSS Models\n‚õèÔ∏è Extraction\n‚õèÔ∏èSummarization and tagging\nü§ñ Agents\nüö® Safety and evaluation\nCommunity\nDiscord\nTwitter\nGitHub\nPython\nJS/TS\nMore\nHomepage\nBlog\nCopyright ¬© 2024 LangChain, Inc."
  },
  {
    "title": "Welcome Contributors | ü¶úÔ∏èüîó Langchain",
    "url": "https://python.langchain.com/docs/contributing",
    "html": "Skip to main content\nü¶úÔ∏èüîó LangChain\nDocs\nUse cases\nIntegrations\nGuides\nAPI\nMore\nü¶úÔ∏èüîó\nChat\nSearch\nK\nWelcome Contributors\nContribute Code\nTesting\nContribute Documentation\nContribute Integrations\nFAQ\nWelcome Contributors\nWelcome Contributors\n\nHi there! Thank you for even being interested in contributing to LangChain. As an open-source project in a rapidly developing field, we are extremely open to contributions, whether they involve new features, improved infrastructure, better documentation, or bug fixes.\n\nüó∫Ô∏è Guidelines‚Äã\nüë©‚Äçüíª Ways to contribute‚Äã\n\nThere are many ways to contribute to LangChain. Here are some common ways people contribute:\n\nDocumentation: Help improve our docs, including this one!\nCode: Help us write code, fix bugs, or improve our infrastructure.\nIntegrations: Help us integrate with your favorite vendors and tools.\nüö©GitHub Issues‚Äã\n\nOur issues page is kept up to date with bugs, improvements, and feature requests.\n\nThere is a taxonomy of labels to help with sorting and discovery of issues of interest. Please use these to help organize issues.\n\nIf you start working on an issue, please assign it to yourself.\n\nIf you are adding an issue, please try to keep it focused on a single, modular bug/improvement/feature. If two issues are related, or blocking, please link them rather than combining them.\n\nWe will try to keep these issues as up-to-date as possible, though with the rapid rate of development in this field some may get out of date. If you notice this happening, please let us know.\n\nüôãGetting Help‚Äã\n\nOur goal is to have the simplest developer setup possible. Should you experience any difficulty getting setup, please contact a maintainer! Not only do we want to help get you unblocked, but we also want to make sure that the process is smooth for future contributors.\n\nIn a similar vein, we do enforce certain linting, formatting, and documentation standards in the codebase. If you are finding these difficult (or even just annoying) to work with, feel free to contact a maintainer for help - we do not want these to get in the way of getting good code into the codebase.\n\nüåü Recognition\n\nIf your contribution has made its way into a release, we will want to give you credit on Twitter (only if you want though)! If you have a Twitter account you would like us to mention, please let us know in the PR or through another means.\n\nNext\nContribute Code\nüó∫Ô∏è Guidelines\nüë©‚Äçüíª Ways to contribute\nüö©GitHub Issues\nüôãGetting Help\nCommunity\nDiscord\nTwitter\nGitHub\nPython\nJS/TS\nMore\nHomepage\nBlog\nCopyright ¬© 2024 LangChain, Inc."
  },
  {
    "title": "Changelog | ü¶úÔ∏èüîó Langchain",
    "url": "https://python.langchain.com/docs/changelog",
    "html": "Skip to main content\nü¶úÔ∏èüîó LangChain\nDocs\nUse cases\nIntegrations\nGuides\nAPI\nMore\nü¶úÔ∏èüîó\nChat\nSearch\nK\nChangelog\nlangchain-core\nlangchain\nChangelog\nChangelog\nüìÑÔ∏è langchain-core\n\n0.1.7 (Jan 5, 2024)\n\nüìÑÔ∏è langchain\n\n0.1.0 (Jan 5, 2024)\n\nNext\nlangchain-core\nCommunity\nDiscord\nTwitter\nGitHub\nPython\nJS/TS\nMore\nHomepage\nBlog\nCopyright ¬© 2024 LangChain, Inc."
  },
  {
    "title": "üìï Package Versioning | ü¶úÔ∏èüîó Langchain",
    "url": "https://python.langchain.com/docs/packages",
    "html": "Skip to main content\nü¶úÔ∏èüîó LangChain\nDocs\nUse cases\nIntegrations\nGuides\nAPI\nMore\nü¶úÔ∏èüîó\nChat\nSearch\nK\nüìï Package Versioning\n\nAs of now, LangChain has an ad hoc release process: releases are cut with high frequency by a maintainer and published to PyPI. The different packages are versioned slightly differently.\n\nlangchain-core‚Äã\n\nlangchain-core is currently on version 0.1.x.\n\nAs langchain-core contains the base abstractions and runtime for the whole LangChain ecosystem, we will communicate any breaking changes with advance notice and version bumps. The exception for this is anything marked with the beta decorator (you can see this in the API reference and will see warnings when using such functionality). The reason for beta features is that given the rate of change of the field, being able to move quickly is still a priority.\n\nMinor version increases will occur for:\n\nBreaking changes for any public interfaces marked as beta.\n\nPatch version increases will occur for:\n\nBug fixes\nNew features\nAny changes to private interfaces\nAny changes to beta features\nlangchain‚Äã\n\nlangchain is currently on version 0.1.x\n\nMinor version increases will occur for:\n\nBreaking changes for any public interfaces NOT marked as beta.\n\nPatch version increases will occur for:\n\nBug fixes\nNew features\nAny changes to private interfaces\nAny changes to beta features\n\nWe are targeting February 2024 for a release of langchain v0.2, which will have some breaking changes to legacy Chains and Agents. Additionally, we will remove langchain-community as a dependency and stop re-exporting integrations that have been moved to langchain-community.\n\nlangchain-community‚Äã\n\nlangchain-community is currently on version 0.0.x\n\nAll changes will be accompanied by a patch version increase.\n\nlangchain-experimental‚Äã\n\nlangchain-experimental is currently on version 0.0.x\n\nAll changes will be accompanied by a patch version increase.\n\nPartner Packages‚Äã\n\nPartner packages are versioned independently.\n\nlangchain-core\nlangchain\nlangchain-community\nlangchain-experimental\nPartner Packages\nCommunity\nDiscord\nTwitter\nGitHub\nPython\nJS/TS\nMore\nHomepage\nBlog\nCopyright ¬© 2024 LangChain, Inc."
  },
  {
    "title": "Debugging | ü¶úÔ∏èüîó Langchain",
    "url": "https://python.langchain.com/docs/guides/debugging",
    "html": "Skip to main content\nü¶úÔ∏èüîó LangChain\nDocs\nUse cases\nIntegrations\nGuides\nAPI\nMore\nü¶úÔ∏èüîó\nChat\nSearch\nK\nDebugging\nDeployment\nEvaluation\nFallbacks\nRun LLMs locally\nModel comparison\nPrivacy\nPydantic compatibility\nSafety\nDebugging\nDebugging\n\nIf you're building with LLMs, at some point something will break, and you'll need to debug. A model call will fail, or the model output will be misformatted, or there will be some nested model calls and it won't be clear where along the way an incorrect output was created.\n\nHere are a few different tools and functionalities to aid in debugging.\n\nTracing‚Äã\n\nPlatforms with tracing capabilities like LangSmith and WandB are the most comprehensive solutions for debugging. These platforms make it easy to not only log and visualize LLM apps, but also to actively debug, test and refine them.\n\nFor anyone building production-grade LLM applications, we highly recommend using a platform like this.\n\nset_debug and set_verbose‚Äã\n\nIf you're prototyping in Jupyter Notebooks or running Python scripts, it can be helpful to print out the intermediate steps of a Chain run.\n\nThere are a number of ways to enable printing at varying degrees of verbosity.\n\nLet's suppose we have a simple agent, and want to visualize the actions it takes and tool outputs it receives. Without any debugging, here's what we see:\n\nfrom langchain.agents import AgentType, initialize_agent, load_tools\nfrom langchain_openai import ChatOpenAI\n\nllm = ChatOpenAI(model_name=\"gpt-4\", temperature=0)\ntools = load_tools([\"ddg-search\", \"llm-math\"], llm=llm)\nagent = initialize_agent(tools, llm, agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION)\n\nagent.run(\"Who directed the 2023 film Oppenheimer and what is their age? What is their age in days (assume 365 days per year)?\")\n\n    'The director of the 2023 film Oppenheimer is Christopher Nolan and he is approximately 19345 days old in 2023.'\n\nset_debug(True)‚Äã\n\nSetting the global debug flag will cause all LangChain components with callback support (chains, models, agents, tools, retrievers) to print the inputs they receive and outputs they generate. This is the most verbose setting and will fully log raw inputs and outputs.\n\nfrom langchain.globals import set_debug\n\nset_debug(True)\n\nagent.run(\"Who directed the 2023 film Oppenheimer and what is their age? What is their age in days (assume 365 days per year)?\")\n\nConsole output\nset_verbose(True)‚Äã\n\nSetting the verbose flag will print out inputs and outputs in a slightly more readable format and will skip logging certain raw outputs (like the token usage stats for an LLM call) so that you can focus on application logic.\n\nfrom langchain.globals import set_verbose\n\nset_verbose(True)\n\nagent.run(\"Who directed the 2023 film Oppenheimer and what is their age? What is their age in days (assume 365 days per year)?\")\n\nConsole output\nChain(..., verbose=True)‚Äã\n\nYou can also scope verbosity down to a single object, in which case only the inputs and outputs to that object are printed (along with any additional callbacks calls made specifically by that object).\n\n# Passing verbose=True to initialize_agent will pass that along to the AgentExecutor (which is a Chain).\nagent = initialize_agent(\n    tools, \n    llm, \n    agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION,\n    verbose=True,\n)\n\nagent.run(\"Who directed the 2023 film Oppenheimer and what is their age? What is their age in days (assume 365 days per year)?\")\n\nConsole output\nOther callbacks‚Äã\n\nCallbacks are what we use to execute any functionality within a component outside the primary component logic. All of the above solutions use Callbacks under the hood to log intermediate steps of components. There are a number of Callbacks relevant for debugging that come with LangChain out of the box, like the FileCallbackHandler. You can also implement your own callbacks to execute custom functionality.\n\nSee here for more info on Callbacks, how to use them, and customize them.\n\nNext\nDeployment\nTracing\nset_debug and set_verbose\nset_debug(True)\nset_verbose(True)\nChain(..., verbose=True)\nOther callbacks\nCommunity\nDiscord\nTwitter\nGitHub\nPython\nJS/TS\nMore\nHomepage\nBlog\nCopyright ¬© 2024 LangChain, Inc."
  },
  {
    "title": "Q&A with RAG | ü¶úÔ∏èüîó Langchain",
    "url": "https://python.langchain.com/docs/use_cases/question_answering/",
    "html": "Skip to main content\nü¶úÔ∏èüîó LangChain\nDocs\nUse cases\nIntegrations\nGuides\nAPI\nMore\nü¶úÔ∏èüîó\nChat\nSearch\nK\nUse cases\nQ&A with RAG\nQuickstart\nReturning sources\nAdd chat history\nStreaming\nPer-User Retrieval\nUsing agents\nUsing local models\nQ&A over structured data\nInteracting with APIs\nChatbots\nExtraction\nSummarization\nTagging\nWeb scraping\nCode understanding\nSynthetic data generation\nGraph querying\nUse casesQ&A with RAG\nQ&A with RAG\nOverview‚Äã\n\nOne of the most powerful applications enabled by LLMs is sophisticated question-answering (Q&A) chatbots. These are applications that can answer questions about specific source information. These applications use a technique known as Retrieval Augmented Generation, or RAG.\n\nWhat is RAG?‚Äã\n\nRAG is a technique for augmenting LLM knowledge with additional data.\n\nLLMs can reason about wide-ranging topics, but their knowledge is limited to the public data up to a specific point in time that they were trained on. If you want to build AI applications that can reason about private data or data introduced after a model‚Äôs cutoff date, you need to augment the knowledge of the model with the specific information it needs. The process of bringing the appropriate information and inserting it into the model prompt is known as Retrieval Augmented Generation (RAG).\n\nLangChain has a number of components designed to help build Q&A applications, and RAG applications more generally.\n\nNote: Here we focus on Q&A for unstructured data. Two RAG use cases which we cover elsewhere are:\n\nQ&A over structured data (e.g., SQL)\nQ&A over code (e.g., Python)\nRAG Architecture‚Äã\n\nA typical RAG application has two main components:\n\nIndexing: a pipeline for ingesting data from a source and indexing it. This usually happens offline.\n\nRetrieval and generation: the actual RAG chain, which takes the user query at run time and retrieves the relevant data from the index, then passes that to the model.\n\nThe most common full sequence from raw data to answer looks like:\n\nIndexing‚Äã\nLoad: First we need to load our data. This is done with DocumentLoaders.\nSplit: Text splitters break large Documents into smaller chunks. This is useful both for indexing data and for passing it in to a model, since large chunks are harder to search over and won‚Äôt fit in a model‚Äôs finite context window.\nStore: We need somewhere to store and index our splits, so that they can later be searched over. This is often done using a VectorStore and Embeddings model.\n\nRetrieval and generation‚Äã\nRetrieve: Given a user input, relevant splits are retrieved from storage using a Retriever.\nGenerate: A ChatModel / LLM produces an answer using a prompt that includes the question and the retrieved data\n\nTable of contents‚Äã\nQuickstart: We recommend starting here. Many of the following guides assume you fully understand the architecture shown in the Quickstart.\nReturning sources: How to return the source documents used in a particular generation.\nStreaming: How to stream final answers as well as intermediate steps.\nAdding chat history: How to add chat history to a Q&A app.\nPer-user retrieval: How to do retrieval when each user has their own private data.\nUsing agents: How to use agents for Q&A.\nUsing local models: How to use local models for Q&A.\nPrevious\nUse cases\nNext\nQuickstart\nOverview\nWhat is RAG?\nRAG Architecture\nTable of contents\nCommunity\nDiscord\nTwitter\nGitHub\nPython\nJS/TS\nMore\nHomepage\nBlog\nCopyright ¬© 2024 LangChain, Inc."
  },
  {
    "title": "Providers | ü¶úÔ∏èüîó Langchain",
    "url": "https://python.langchain.com/docs/integrations/providers",
    "html": "Skip to main content\nü¶úÔ∏èüîó LangChain\nDocs\nUse cases\nIntegrations\nGuides\nAPI\nMore\nü¶úÔ∏èüîó\nChat\nSearch\nK\nProviders\nAnthropic\nAWS\nGoogle\nHugging Face\nMicrosoft\nOpenAI\nMore\nComponents\nLLMs\nChat models\nDocument loaders\nDocument transformers\nText embedding models\nVector stores\nRetrievers\nTools\nAgents and toolkits\nMemory\nCallbacks\nChat loaders\nAdapters\nStores\nProviders\nProviders\nüìÑÔ∏è Anthropic\n\nAll functionality related to Anthropic models.\n\nüìÑÔ∏è AWS\n\nThe LangChain integrations related to Amazon AWS platform.\n\nüìÑÔ∏è Google\n\nAll functionality related to Google Cloud Platform and other Google products.\n\nüìÑÔ∏è Hugging Face\n\nAll functionality related to the Hugging Face Platform.\n\nüìÑÔ∏è Microsoft\n\nAll functionality related to Microsoft Azure and other Microsoft products.\n\nüìÑÔ∏è OpenAI\n\nAll functionality related to OpenAI\n\nüóÉÔ∏è More\n\n201 items\n\nNext\nAnthropic\nCommunity\nDiscord\nTwitter\nGitHub\nPython\nJS/TS\nMore\nHomepage\nBlog\nCopyright ¬© 2024 LangChain, Inc."
  },
  {
    "title": "Introduction | ü¶úÔ∏èüîó Langchain",
    "url": "https://python.langchain.com/docs/get_started/introduction",
    "html": "Skip to main content\nü¶úÔ∏èüîó LangChain\nDocs\nUse cases\nIntegrations\nGuides\nAPI\nMore\nü¶úÔ∏èüîó\nChat\nSearch\nK\nGet started\nIntroduction\nInstallation\nQuickstart\nSecurity\nLangChain Expression Language\nGet started\nWhy use LCEL\nInterface\nHow to\nCookbook\nModules\nModel I/O\nRetrieval\nAgents\nChains\nMore\nLangServe\nLangSmith\nLangGraph\nGet startedIntroduction\nIntroduction\n\nLangChain is a framework for developing applications powered by language models. It enables applications that:\n\nAre context-aware: connect a language model to sources of context (prompt instructions, few shot examples, content to ground its response in, etc.)\nReason: rely on a language model to reason (about how to answer based on provided context, what actions to take, etc.)\n\nThis framework consists of several parts.\n\nLangChain Libraries: The Python and JavaScript libraries. Contains interfaces and integrations for a myriad of components, a basic run time for combining these components into chains and agents, and off-the-shelf implementations of chains and agents.\nLangChain Templates: A collection of easily deployable reference architectures for a wide variety of tasks.\nLangServe: A library for deploying LangChain chains as a REST API.\nLangSmith: A developer platform that lets you debug, test, evaluate, and monitor chains built on any LLM framework and seamlessly integrates with LangChain.\n\nTogether, these products simplify the entire application lifecycle:\n\nDevelop: Write your applications in LangChain/LangChain.js. Hit the ground running using Templates for reference.\nProductionize: Use LangSmith to inspect, test and monitor your chains, so that you can constantly improve and deploy with confidence.\nDeploy: Turn any chain into an API with LangServe.\nLangChain Libraries‚Äã\n\nThe main value props of the LangChain packages are:\n\nComponents: composable tools and integrations for working with language models. Components are modular and easy-to-use, whether you are using the rest of the LangChain framework or not\nOff-the-shelf chains: built-in assemblages of components for accomplishing higher-level tasks\n\nOff-the-shelf chains make it easy to get started. Components make it easy to customize existing chains and build new ones.\n\nThe LangChain libraries themselves are made up of several different packages.\n\nlangchain-core: Base abstractions and LangChain Expression Language.\nlangchain-community: Third party integrations.\nlangchain: Chains, agents, and retrieval strategies that make up an application's cognitive architecture.\nGet started‚Äã\n\nHere‚Äôs how to install LangChain, set up your environment, and start building.\n\nWe recommend following our Quickstart guide to familiarize yourself with the framework by building your first LangChain application.\n\nRead up on our Security best practices to make sure you're developing safely with LangChain.\n\nNOTE\n\nThese docs focus on the Python LangChain library. Head here for docs on the JavaScript LangChain library.\n\nLangChain Expression Language (LCEL)‚Äã\n\nLCEL is a declarative way to compose chains. LCEL was designed from day 1 to support putting prototypes in production, with no code changes, from the simplest ‚Äúprompt + LLM‚Äù chain to the most complex chains.\n\nOverview: LCEL and its benefits\nInterface: The standard interface for LCEL objects\nHow-to: Key features of LCEL\nCookbook: Example code for accomplishing common tasks\nModules‚Äã\n\nLangChain provides standard, extendable interfaces and integrations for the following modules:\n\nModel I/O‚Äã\n\nInterface with language models\n\nRetrieval‚Äã\n\nInterface with application-specific data\n\nAgents‚Äã\n\nLet models choose which tools to use given high-level directives\n\nExamples, ecosystem, and resources‚Äã\nUse cases‚Äã\n\nWalkthroughs and techniques for common end-to-end use cases, like:\n\nDocument question answering\nChatbots\nAnalyzing structured data\nand much more...\nIntegrations‚Äã\n\nLangChain is part of a rich ecosystem of tools that integrate with our framework and build on top of it. Check out our growing list of integrations.\n\nGuides‚Äã\n\nBest practices for developing with LangChain.\n\nAPI reference‚Äã\n\nHead to the reference section for full documentation of all classes and methods in the LangChain and LangChain Experimental Python packages.\n\nDeveloper's guide‚Äã\n\nCheck out the developer's guide for guidelines on contributing and help getting your dev environment set up.\n\nCommunity‚Äã\n\nHead to the Community navigator to find places to ask questions, share feedback, meet other developers, and dream about the future of LLM‚Äôs.\n\nPrevious\nGet started\nNext\nInstallation\nLangChain Libraries\nGet started\nLangChain Expression Language (LCEL)\nModules\nExamples, ecosystem, and resources\nUse cases\nIntegrations\nGuides\nAPI reference\nDeveloper's guide\nCommunity\nCommunity\nDiscord\nTwitter\nGitHub\nPython\nJS/TS\nMore\nHomepage\nBlog\nCopyright ¬© 2024 LangChain, Inc."
  }
]